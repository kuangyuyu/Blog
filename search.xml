<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[剑指offer（三）：14-18]]></title>
    <url>%2Farchives%2F1084f876.html</url>
    <content type="text"><![CDATA[剑14-反转链表输入一个链表，反转链表后，输出链表的所有元素。1234567891011121314151617class ListNode: def_init_(self,x): self.val = x self.next = Noneclass Solution: def ReverseList(self,pHead): #判断指针是否为空 if pHead == None or pHead.next == None: return pHead #反转链表 last = Nonep while pHead != None: tmp = pHead.next pHead.next = last last = pHead pHead = tmp return last 剑15-合并两个排序的链表输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 思路：比较两个链表的首结点，哪个小的的结点则合并到第三个链表尾结点，并向前移动一个结点。 步骤一结果会有一个链表先遍历结束，或者没有第三个链表尾结点指向剩余未遍历结束的链表 返回第三个链表首结点 分析： 123456789101112131415161718192021class ListNode: def __init__(self,x): self.val = x self.next = Noneclass Solution: def Merge(self, pHead1,pHead2): new = ListNode(1) last = new while pHead1 and pHead2: if pHead1.val &lt;= pHead2.val: new.next = pHead1 pHead1 = pHead1.next else: new.next = pHead2 pHead2 = pHead2.next new = new.next if pHead1: new.next = pHead1 else: new.next = pHead2 return last.next 剑16-树的子结构输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构）分析： 123456789101112class Solution: def HasSubtree(self, pRoot1, pRoot2): # write code here if not pRoot1 or not pRoot2: return False return self.is_subtree(pRoot1, pRoot2) or self.HasSubtree(pRoot1.left, pRoot2) or self.HasSubtree(pRoot1.right, pRoot2) def is_subtree(self, A, B): if not B: return True if not A or A.val != B.val: return False return self.is_subtree(A.left,B.left) and self.is_subtree(A.right, B.right) 剑17-二叉树的镜像题目描述操作给定的二叉树，将其变换为源二叉树的镜像。输入描述:二叉树的镜像定义：源二叉树 8 / \ 6 10 / \ / \ 5 7 9 11 镜像二叉树 8 / \ 10 6 / \ / \ 11 9 7 5 12345678910111213# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回镜像树的根节点 def Mirror(self,root): if root != None: root.left,root.right=root.right,root.left self.Mirror(root.left) self.Mirroe(root.right) 剑18-顺时针打印矩阵输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下矩阵： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 则依次打印出数字1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10. 12345678910111213141516def printMatrix(self, matrix): if not matrix: return [] if len(matrix)==1: return matrix[0] target = [] while (1): target.extend(matrix[0]) del matrix[0] if len(matrix)==0: break tmp=[] for j in range(1,len(matrix[0])+1): tmp.append([m[-j] for m in matrix]) matrix=tmp return target 我觉得下面这个比较好理解一些： 可以模拟魔方逆时针旋转的方法，一直做取出第一行的操作例如1 2 34 5 67 8 9输出并删除第一行后，再进行一次逆时针旋转，就变成：6 95 84 7继续重复上述操作即可。Python代码如下1234567891011121314151617181920212223 class Solution: # matrix类型为二维列表，需要返回列表 def printMatrix(self, matrix): # write code here result = [] while(matrix): result+=matrix.pop(0) if not matrix or not matrix[0]: break matrix = self.turn(matrix) return result def turn(self,matrix): num_r = len(matrix) num_c = len(matrix[0]) newmat = [] for i in range(num_c): newmat2 = [] for j in range(num_r): newmat2.append(matrix[j][i]) newmat.append(newmat2) newmat.reverse() return newmat]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>查找</tag>
        <tag>替换空格</tag>
        <tag>打印链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法：GAN]]></title>
    <url>%2Farchives%2Fb2b9d295.html</url>
    <content type="text"><![CDATA[前言生成对抗网络（GAN）一经提出就风光无限，更是被Yann Lecun誉为“十年来机器学习领域最有趣的想法”。 GAN“左右互搏”的理念几乎众所周知，但正如卷积神经网络（CNN）一样，GAN发展至今已经衍生出了诸多变化形态。 今天，文摘菌就来为大家盘点一下GAN大家庭中各具特色的成员们。 他们的名单如下： 1.DCGANs 2.Improved DCGANs 3.Conditional GANs 4.InfoGANs 5.Wasserstein GANs 6.Improved WGANs 7.BEGANs 8.ProGANs 9.CycleGANs 注意，这篇文章不会包含以下内容 • 复杂的技术分析 • 代码（但有代码链接） • 详细的研究清单 （你可以点击以下链接https://github.com/zhangqianhui/AdversarialNetsPapers） 想要了解更多GANs相关内容的也可以留言告诉文摘菌哦～ GANs概论如果你对GANs很熟悉的话，你可以跳过这部分的内容。 GANs最早由Ian Goodfellow提出，由两个网络构成，一个生成器和一个鉴别器。他们在同一时间训练并且在极小化极大算法（minimax）中进行竞争。生成器被训练来欺骗鉴别器以产生逼真的图像，鉴别器则在训练中学会不被生成器愚弄。 GAN 训练原理概览 首先，生成器通过从一个简单分布（例如正态分布）中抽取一个噪音向量Z，并且上行采样（upsample）这个向量来生成图像。在最初的循环中，这些图像看起来非常嘈杂。然后，鉴别器得到真伪图像并学习去识别它们。随后生成器通过反向传播算法（backpropagation）收到鉴别器的反馈，渐渐在生成图像时做得更好。我们最终希望伪图像的分布尽可能地接近真图像。或者，简单来说，我们想要伪图像尽可能看起来像真的一样。 值得一提的是，因为GANs是用极小化极大算法做优化的，所以训练过程可能会很不稳定。不过你可以使用一些“小技巧”来获得更稳健的训练过程。 在下面这个视频中，你可以看到GANs所生成图片的训练演变过程。 代码如果对GANs的基本实现感兴趣，可以参见代码的链接： Tensorflow（https://github.com/ericjang/genadv_tutorial/blob/master/genadv1.ipynb） Torch 和 Python (PyTorch)（https://github.com/devnag/pytorch-generative-adversarial-networks；https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f） Torch 和 Lua（https://github.com/lopezpaz/metal） 虽然这些不是最前沿的内容，但它们对于掌握理念很有帮助。 接下来我将按照粗略的时间顺序描述最近这些年来出现的GANs相关的一些进展和类型。 深度卷积生成式对抗网络（Deep Convolutional GANs, DCGANs）DCGANs是GAN结构的最早的重要发展。就训练和生成更高质量的样本来说，DCGANs更加稳定。 论文链接：https://arxiv.org/abs/1511.06434 DCGAN的作者们专注于提升初代GAN的框架结构。他们发现： 批次（Batch）的正态化在两个网络中都是必须要做的。 完全隐藏的连接层不是一个好的想法。 避免池化，只需卷积 修正线性单元（ReLU）激活函数非常有用。 DCGANs截至目前任然被时常提起，因为它们成为了实践和使用GANs的主要基准之一。 在这篇论文发布后不久，就在Theano, Torch, Tensorflow 和 Chainer 中出现了不同的可使用的实现方法，这些方法可以在你感兴趣的任何数据集上测试。所以，如果你遇到了生成后奇怪的数据集，你完全可以归咎于这些家伙。 DCGANs的使用场景如下：你想要比基础GANs表现更好（这是必须的）。基础GANs适用于简单的数据集，然而DCGANs比这要强得多。 你在寻找一种稳固的基准方法来比较你的新GAN算法。 从现在开始，除非特别说明，我接下来将要描述的所有GANs的类型都被假定为有DCGAN的结构。 提升深度卷积生成式对抗网络（Improved DCGANs）一系列提升先前的DCGAN的技术。例如，这一提升后的基准方法能够生成更好的高分辨率图像。 论文链接：https://arxiv.org/abs/1606.03498 GANs的主要问题之一是收敛性。收敛性不是一定的，而且尽管DCGAN做了结构细化，训练过程仍可能非常不稳定。 在这篇论文里，作者们针对GAN训练过程提出了不同的增强方法。以下是其中一部分： 特征匹配：他们提出了一种新的目标函数，而不是让生成器尽可能地去蒙骗鉴别器。这个目标函数需要生成器生成的数据能够跟真实数据的统计量相匹配。在这种情况下，鉴别器只被用来指定哪些才是值得去匹配的统计量。 历史平均：在更新参数值时，把它们过去的值也纳入考虑。 单侧标签平滑：这一项非常简单：只要把你的鉴别器的目标输出值从[0=假图像，1=真图像]改成[0=假图像，0.9=真图像]。不错，这样就提升了训练效果。 虚拟批次正态化：通过使用从其他批次计算的统计量来避免依赖于同一批次的数据。这样的计算成本很高，所以它仅仅被用在生成器当中。 所有这些技术都使得模型在生成高分辨率图像时能表现得更好，而这正是GANs的弱项之一。 作为对比，请看在128x128图像上原始DCGAN和提升后的DCGAN的表现差异： 这些本来都是狗的图片。正如你看到的，DCGAN表现很糟糕，而用improved DCGAN你至少可以看到一些包含狗的特征的内容。这也说明了GANs的另一局限——生成结构性的内容。 Improved DCGANs的使用场景如下生成更高分辨率的图像 条件生成式对抗网络（Conditional GANs， cGANs)条件式生成式对抗网络使用额外的标签信息用于生成更高质量图片，并且使图片的呈现可控制。 论文链接：https://arxiv.org/abs/1411.1784 CGANs是GAN框架的扩展。我们用条件信息Y来描述数据的某些特征。假设我们要处理面部图像，Y则可以用来描述头发颜色或者性别。然后这些属性被插入生成器和鉴别器。 使用脸部特征信息的条件生成网络如上图所示 条件式生成对抗网络有两个很有意思的地方： 1、随着你不断给模型提供更多信息，GAN学习探索这些信息，然后产生更好的样本。 2、我们用两种方法来控制图片的呈现，在没有CGAN的时候所有图片信息使用Z编码。在CGAN下，我们加入了条件信息Y，于是Z和Y对不同信息进行编码。 例如，我们假设Y对手写数字0-9进行编码。Z对其他变量编码，这些变量可以是数字的风格比如（大小，粗细，旋转角度等。） MNIST(Mixed National Institute of Standards and Technology database,简单机器视觉数据集)样本中Z和Y的区别如上图。Z是行，Y是列；Z对数字的风格编码，Y对数字本身编码。 最近的研究成果在这个领域有很多有趣的文章，我介绍2个： 1、使用生成对抗网络学习在指定位置画画 （论文：https://arxiv.org/abs/1610.02454；代码：https://github.com/reedscot/nips2016）：这篇论文里作者设计了一个本文描述的方法来告诉GAN画什么，同时使用方框和标记告诉GAN绘画主体的位置。如下图示： 堆栈式GAN （原文：https://arxiv.org/abs/1612.03242；代码：https://github.com/hanzhanggit/StackGAN） 这篇文章和上一篇比较类似，这里作者同时使用2个GAN网络（阶段1和阶段2）用于提升图片的质量。第1阶段用来获得包含图片“基本”概念的低分辨率图片。第2阶段用更多的细节和更高的分辨率提炼第1阶段的图片。 这篇文章据我所知是生成高质量图片最好的模型之一，不信请看下图。 条件式生成网络的使用场景如下：1、你有一个带标签的训练集，想提高生成图片的质量 2、你想对图片的某些特征进行精细的控制，比如在某个设定的位置生成特定大小的一只红色小鸟。 最大信息化生成对抗网络（InfoGANs）GANs可以在无监督模式下对噪声向量Z的一部分有意义的图像特征进行编码。比如对某一数字的旋转编码。 论文链接：https://arxiv.org/abs/1606.03657 你是否曾想过GAN里输入噪声Z对哪些信息进行编码？一般来说它对图片的不同类型的特征使用一种非常“嘈杂”的方式编码。例如，取Z向量的一个位置 并且插入一个-1到1之间的值。这是下图所示的MNIST数据集的训练模型。 左上图像是对Z插值取-1的时候，右下是插值为1的时候 上图中，生成的图片看似是从4慢慢变成“Y”（很像是4和9的混合体）。 这就是我之前所说的使用一种“嘈杂”的方式进行编码：Z的一个位置是图像多个特征的一个参数。 这种情况下，这个位置改变了数字自己（某种程度，从4变成9）和他的风格（从粗体变成斜体）。 然而，你无法定义Z的位置的确切含义。 如果使用Z的一些位置来代表唯一且受限的信息，正如CGAN里的条件信息Y呢？ 例如，第一个位置是0-9之间的数值来控制数字，第二个位置来控制数字的旋转，这就是文中作者想要表达的。 有意思的是，与CGANS不同，他们使用无监督的方法实现并不需要标签信息。 他们是这么做的，把Z向量拆分成两部分：C和Z C对数据分布的语义特征进行编码 Z对分布的所有非结构化噪声进行编码 如何用C对这些特征编码呢？ 通过改变损失函数避免C被GAN简单地忽略掉。所以他们使用一种信息论正则化确保C与生成器分布之间的互信息[z1] （mutual information）。 也就是说，如果C变化，生成的图像也会变化。这导致你不能明确的控制什么类型的信息将被引入C中。但是C的每一个位置都有独特的含义。 如下图所示： C的第一位置编码数字的类别，第二位置编码旋转方向。 然而，不使用标签信息的代价是，这些编码仅对非常简单的数据集有效比如MNIST库里的数字。 并且，你还需要手工设定C的每个位置。例如文章中作者需要定义C的第一位置是介于0-9的整数以对应数据集的十类数字。你会认为这样不是百分百的无监督，因为需要手动给模型提供一些细节。 你可能需要用到infoGANs的场景如下： 1、数据集不太复杂 2、你想训练一个CGAN模型但是缺少标签信息 3、你想知道数据集主要有意义的图像特征并且对他们进行控制 生成式对抗网络（Wasserstein GANs）修改损失函数以引入Wasserstein距离，这样以来WassGANs 的损失函数同图片质量建立联系。同时训练的稳定性有所提升而并非依赖于架构。 论文链接：https://arxiv.org/abs/1701.07875 GANs 经常存在收敛问题，所以你并不知道什么时候该停止训练。换句话说损失函数同图像质量无关，这可是个大难题。 因为： 你需要不断的查看样本来确认模型训练是否正确 因为不收敛，你不知道何时该停止训练 也没有数值指示你调参的效果如何 举个例子，看下面DCGAN的两个无信息损失函数完美的生成MNIST样本的迭代图[z2] 仅看上图你知道什么时候该停止训练吗？ 这种可解释性的问题也是Wasserstein GANs要解决的。 怎样解决呢？ 如果真实和伪造样本的分布不重叠（一般都是这样）GANs可以用来最小化JS散度(Jensen-Shannon divergence)直到0。 所以与其最小化JS散度，作者使用Wasserstein距离来描述不同分布中点之间的距离。 思路大概如此，如果你想了解更多，我强烈建议你看这篇文章（https://paper.dropbox.com/doc/Wasserstein-GAN-GvU0p2V9ThzdwY3BbhoP7）或者更深入分析或者阅读本论文。 WassGAN有和图像质量关联的损失函数并且能够收敛。同时他更加稳定而不依赖GAN的结构。比如，就算你去掉批标准化或者使用怪异的结构，它仍能很好的工作。 这就是他的损失函数图，损失越低，图像质量越好。完美！ Wasserstein GANs的应用场景如下： 你需要寻找最先进的且有最高训练稳定性的GAN 你想要一个有信息量且可以解读的损失函数。 加强版WGANs (Improved WGANs , WGAN-GP)这个模型使用的是Wasserstein GANs，并应用了梯度惩罚（gradient penalty）来代替权重剪辑（weight clipping）及其带来的一些不需要的行为。这个方法可以带来更高的聚合度、更高质量的样本和更稳定的训练。 论文：https://arxiv.org/abs/1704.00028 代码：https://github.com/igul222/improved_wgan_training 针对问题：WGANs有时候会生成一些质量不佳的样本，或者是无法在某些集合中生成聚集。这种现象主要是由于为了满足Lipschitz限制而在WGANs中应用权重剪辑（即把所有权重限制在一个由最小值和最大值组成的范围内）所造成的。如果你对这个限制不太了解，那么你只需要记住它是WGANs正常运行的一个必要条件。那么为什么权重剪辑会造成如上问题呢？这是因为它会使WGANs偏向于使用那些过于简单的函数，这意味着当WGANs想要模拟复杂的数据时，简单地估算近似值令其无法得到准确的结果（如下图所示）。另外，权重剪辑也使得梯度爆炸与消失更容易发生。 左图是使用了简单的函数导致无法正确模拟高斯为8的WGANs运行后的结果，而右图则是经过使用了更复杂的函数的WGAN-GP矫正后的图像。 梯度惩罚（Gradient penalty）：所以我们该如何摆脱权重剪辑带来的不良效果呢？ WGAN-GP的作者（GP表示梯度惩罚）提出了使用另一种叫梯度惩罚的办法来加强Lipschitz限制的建议。原则上，梯度惩罚的原理是对某些梯度实行了均值为1的限制。对于那些均值偏离于1的梯度将会实施惩罚（减少权重），这也是为什么它被称为梯度惩罚的原因。 优点：由于在训练中使用了梯度惩罚而不是权重剪辑，WGANs获得了更快的聚合。另外，因为不再必须对超参数进行调整，而且网络架构的使用也不再像之前那么重要，训练在某种程度上变得更加稳定。虽然很难说清楚到底有多少，但这些WGAN-GP确实产生了更高质量的样本。在已验证并测试的结构上，这些样本的质量和作为基线的WGANs产出的结果非常相似。 基于同一个网络架构，WGANs-GP明显在生成高质量样本上更有优势，而GANs则不然。 比如，据作者所知，这是首次GANs能够在残差网络的结构上运行： 为了不超出本篇博文的范畴，还有许多其他有趣的细节我这里就不一一阐述了。如果你对这个训练方式有兴趣想要了解更多（例如，为什么梯度惩罚只应用于“某些”特定梯度，又或者怎样才能把这个模型用于文本数据样本），我会建议你自行阅读一下该论文。 强化版的WGAN的优点如下： l 更快的聚合 l 可以在各种各样的网络架构和数据集中使用 l 不像其他的GANs一样需要太多的超参数调整 边界均衡GANs(Boundary Equilibrium GANs，BEGANs)GANs使用一个自动编码器作为均衡判别器。它们可在一个简单的架构上被训练出来，且合成一个动态的可以使得均衡器和生成器在训练中两者平衡的阶段。 论文：https://arxiv.org/abs/1703.10717 一个有趣的事实：BEGANs 和WGAN-GP几乎是同一天在论文上发表。 理念：让BEGAN区别于其他GANs的原因有两个，一是它们使用了一个自动编码器作为均衡判别器（这一点和EBGANs类似），二是为了适用于这个情境的一个特别的损失函数。这种选择背后的理由是什么呢？强迫我们使用像素重建损失函数来制造模糊生成样本的自动编码器是否也不是传说中那么“邪恶”呢？ 要回答这些问题，我们需要考虑以下两点： 为什么要重建像素损失？作者解释说这么做的原因是，对于那些符合对像素损失的重建分布模式的假设，我们可以依赖它们去匹配样本的分布模式。 而这又引出了下一个问题：如何才可以达到这一目的呢？一个重要的观点是，从自动编码器/均衡判别器形成的像素重建损失（换言之，就是基于某个图像输入，输出最优化的像素重建）并不是经BEGANs最小化后生成的最终损失。可以说，这个重建损失不过是为了计算最终损失的其中一个步骤。而最终损失的计算这是通过衡量基于真实数据的重建损失和基于生成数据的重建损失之间的Wasserstein距离（是的，现在它无处不在）。 这么一看似乎这其中的信息量非常大，但我可以保证，一旦我们看清损失函数是如何作用于生成器和判别器，这一切将会变得再明白不过了。 生成器专注于生成那些能够让判别器良好重建的图像 而判别器则致力于尽可能良好地重建真实图像，同时重建那些误差最大化的生成图像。 差异因数：另一个有趣的成分是所谓的差异因数。通过这个因子你能控制判别器不同程度的表现，决定它能只专注于形成对真实图像的完美重建（注重质量），又或是更侧重在区分真实图像和生成图像（注重多样性）。然后，它们就能更进一步地利用这个差异因数去保持生成器和判别器在训练中的平衡。如同WGANs,这一模型同样应用均衡状态作为调整和图像质量相关的聚合度的方法。然而，和WGANs (与 WGANs-GP)不尽相同的是，它们利用了Wasserstein 距离，而非Lipschitz限制，去测量均衡水平。 结论：BEGANs并不需要任何花里胡哨的网络架构就可以训练得当。如同在论文中所述的，“不批量标准化，不丢弃神经网络元，无反卷积，也没有卷积滤镜的指数化增长”，这些生成样本(128x128)的质量已经相当好了。 然而，在这篇论文中有一个重要的细节值得我们注意：论文中使用的未发表数据集的样本量是目前广为使用的 CelebA数据集的两倍之大。因此，为了得到更具有现实意义的定性比较，我邀请大家去查看那些使用了CelebA的公开模型（https://github.com/carpedm20/BEGAN-tensorflow），并看看那些由此生成的样本。 最后，如果你对BEGANs感兴趣想要了解更多，我建议你阅读一下这篇博文（https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/），里面对BEGANs有更多详细的介绍。 你需要BEGANs的原因一般会和需要使用WGANs-GP的情况差不多。这两个模型的结果往往非常相似（稳定的训练、简单的架构、和图像质量相关的损失函数），而两者的不同主要在于过程中使用的手段。由于评估生成式模型本身就不是一件容易的事，我们很难去说清楚孰优孰劣。但就像Theisetal在他们的论文（https://arxiv.org/abs/1511.01844）中所说的，选择一个评估的方法，不然就依据实际情况来做判定。在这种情况下，WGANs-GP具有更高的初始分数（Inception score）而BEGANs则能生成非常优质的图像，两者都是未来最值得研究和最具有革新意义的模型。 渐进式发展生成对抗网络（Progressive growing of GANs，ProGANs）在训练过程中,逐步嵌入新的高分辨率的层次，来生成相当逼真的图像。更多的进步以及新兴的考核机制也相继涌现。新生成的图片质量高得惊人。 论文：https://arxiv.org/abs/1710.10196 代码：https://github.com/tkarras/progressive_growing_of_gans 生成高分辨率的图像是个大挑战。图片越大，对生成对抗网络来说越可能失败。因为它要求模型学习辨别更细微、复杂的细节。为了使大家更好理解，在这篇文章发表之前，GANs产出图像的合理尺寸大概是256x256。渐进式生成对抗网络（ProGANs）将它提升到了一个全新水准-生成尺寸为1024x1024的图片。我们来看看它是如何实现的： 理念: ProGANs-建立于WFGANs-GP-引入了一种灵活地将新层次逐渐嵌入到训练时间的方式。每一层都利用上采样增强了生成器和判别器里的图像分辨率。 步骤分解如下： 1 首先，利用低像素图片训练生成器和判别器。 2 在一定时间点（开始聚合的时候），提高分辨率。这是通过“迁移阶段”/“平滑技术”非常完美地实现的。 新的层次是借助α控制的一系列细微的线性步骤添加，而不是简单粗暴地加上一层。 我们看看生成器里都发生了什么。最初，当α = 0，无变化。产出主要来自于前一低分辨率层（16x16）的贡献。 当提升α的值，新一层（32x32）开始通过反向传播调整它的权重。最后，当α趋近于1，我们几乎可以跳过32x32这一层的操作。同理适用于判别器，当然是以完全相反的方式：不是使图片更大，而是更小。 3 一旦转化完成，继续训练生成器与判别器。假如得到的图片质量分辨率不达标，请回到步骤2。 不过，等等……对新的高分辨率图像的上采样与串联不是已经在StackGANs（还有新StackGANs++）里实现了么？没错，也不全对。首先，StackGANs是将文字翻译为图像的条件GANs，它引入文字描述作为额外输入值。而ProGANs没有使用任何假定式的信息。更加有趣的是，尽管StackGANs和ProGANs都对更高分辨率的图片串联，StackGANs需要尽量多的根据上采样独立配对的GANs-需单独训练，你想进行上采样3次么？那你要训练3个GANs。另一方面，在ProGANs模型中只有一个单一的GAN被训练。在这一训练中，更多的上采样层被逐步叠加至上采样的图片，上采样3次的尝试只是增加更多的层在训练时间上，而不是抓3个新的GANs逐次训练。总而言之，ProGANs采用一种与StackGANs相似的理念，它完美地完成，而且在没有额外信息的情况下产出更好的结果。 结果。作为渐进训练的结果，ProGANs生成的图片有更高的质量，而针对1024x1024图像的训练时间也缩减了5.4倍。背后的理由是，ProGAN不需要一次性学习所有大、小规模的典型图片。在ProGAN模型里，小规模的图像最先被学习（低分辨率层次聚合），然后模型可以自行聚焦在完善大尺寸图片的结构上（新的高分辨率层次聚合）。 其他的提升。 另外，该论文提出了新的设计决策来更进一步提升模型的性能。概述如下： 小批量标准差：将标准差运用于这些小批量的所有特征属性，允许每个小批量有相似的统计数据。然后，总结为一个新的层的单一值，嵌入至网络尾端。 均衡的学习速率：通过在训练过程中持续地将每个权重除以一个常量，确保所有权重的学习速度一致。 像素标准化：在生成器上，每一个矢量的特征是根据其卷基层标准化的（论文里的精确公式）。这是为了防止生成器和判别器的梯度量级的逐步上涨。 CelebA-HQ（CelebA高级版）。 值得一提的是，作者为了实现高分辨率训练，改进了原始的CelebA，从而得到了CelebA-HQ，简单来说，它们移除原像，应用高斯滤波器来生成一种景深效果，然后探测到人脸图像，得到一个1024x1024尺寸的裁剪图片，在这一流程之后，他们从202k张图片里保留了最优质的30k张图片。 评估最后，我们介绍一种新的评估方式： 背后的理念是：生成图像的本地图形结构应该与训练图像的结构匹配。 那么如何测量本地结构？ 使用Laplacian pyramid算法，你可以得到不同的空间频段，来作为描述符。 最后，我们从生成图像和原始图像中提取描述符，将其规范化，然后使用著名的Wasserstein距离检测它们有多接近。距离越近越好。 你可能会想在以下场景使用ProGANs的使用场景如下： 假如你希望获得最完美的结果。但是考虑到… 你需要花大量时间搭建模型：我们需要在一个单一的NVIDIA Tesla P100 GPU上花20天训练网络 如果你开始怀疑世界的真实性。GANs的下一轮迭代可能会给你一些比现实生活还真实的样本参考。 循环生成对抗网络（Cycle GANs）论文：https://arxiv.org/pdf/1703.10593.pdf ​ 代码：https://github.com/junyanz/CycleGAN 循环GANs是目前最先进的用于图片互译的生成对抗网络。 这些GANs并不需要配对的数据集来学习不同领域之间的转译，这点很棒，因为配对数据集很难获取。然而CycleGANs仍然需要通过两个不同领域的数据X和Y（例如X是普通的马匹，Y是斑马）来训练。为了将转换限制于从一个领域到另一领域的范畴，他们使用所谓的“循环一致性损失”。大意是，如果可以将马匹A转化成斑马A，那么当你将斑马A逆转化，应该能得到马匹A才对。 这种从一个领域到另一领域的变换与另一热门概念“神经风格转换”是有区别的。后者结合了一副图像的“内容”与另一图像的“样式”。循环GANs则是在一个领域转换至另一领域的过程中学习总结非常高层次的特征。因此，循环GANs更笼统，并且适用于多种映射关系，例如将一个速写转化为真实物品。 总结一下，生成对抗网络在近期取得了两大显著进步： WGANS-GP和BEGANs 尽管理论研究方向不同，但它们提供了相似的优点。其次，我们有ProGANs（基于WGANS-GP），它打开了一条通往生成高清图像的清晰路径。同时，循环GANs让我们看到了GANs从数据集提取有用信息的潜力以及这些信息如何转入至另一非关联的数据分布。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法：十大算法简介]]></title>
    <url>%2Farchives%2F873fb2b4.html</url>
    <content type="text"><![CDATA[前言在机器学习中，有一种叫做「没有免费的午餐」的定理。简而言之，它指出没有任何一种算法对所有问题都有效，在监督学习（即预测建模）中尤其如此。 例如，你不能说神经网络总是比决策树好，反之亦然。有很多因素在起作用，例如数据集的大小和结构。 因此，你应该针对具体问题尝试多种不同算法，并留出一个数据「测试集」来评估性能、选出优胜者。 当然，你尝试的算法必须适合你的问题，也就是选择正确的机器学习任务。打个比方，如果你需要打扫房子，你可能会用吸尘器、扫帚或拖把，但是你不会拿出铲子开始挖土。 大原则不过也有一个普遍原则，即所有监督机器学习算法预测建模的基础。 机器学习算法被描述为学习一个目标函数 f，该函数将输入变量 X 最好地映射到输出变量 Y：Y = f(X) 这是一个普遍的学习任务，我们可以根据输入变量 X 的新样本对 Y 进行预测。我们不知道函数 f 的样子或形式。如果我们知道的话，我们将会直接使用它，不需要用机器学习算法从数据中学习。 最常见的机器学习算法是学习映射 Y = f(X) 来预测新 X 的 Y。这叫做预测建模或预测分析，我们的目标是尽可能作出最准确的预测。 对于想了解机器学习基础知识的新手，本文将概述数据科学家使用的 top 10 机器学习算法。 线性回归线性回归可能是统计学和机器学习中最知名和最易理解的算法之一。 预测建模主要关注最小化模型误差或者尽可能作出最准确的预测，以可解释性为代价。我们将借用、重用包括统计学在内的很多不同领域的算法，并将其用于这些目的。 线性回归的表示是一个方程，它通过找到输入变量的特定权重（称为系数 B），来描述一条最适合表示输入变量 x 与输出变量 y 关系的直线。 线性回归 例如：y = B0 + B1 * x 我们将根据输入 x 预测 y，线性回归学习算法的目标是找到系数 B0 和 B1 的值。 可以使用不同的技术从数据中学习线性回归模型，例如用于普通最小二乘法和梯度下降优化的线性代数解。 线性回归已经存在了 200 多年，并得到了广泛研究。使用这种技术的一些经验是尽可能去除非常相似（相关）的变量，并去除噪音。这是一种快速、简单的技术，可以首先尝试一下。 Logistic 回归Logistic 回归是机器学习从统计学中借鉴的另一种技术。它是解决二分类问题的首选方法。 Logistic 回归与线性回归相似，目标都是找到每个输入变量的权重，即系数值。与线性回归不同的是，Logistic 回归对输出的预测使用被称为 logistic 函数的非线性函数进行变换。 logistic 函数看起来像一个大的 S，并且可以将任何值转换到 0 到 1 的区间内。这非常实用，因为我们可以规定 logistic 函数的输出值是 0 和 1（例如，输入小于 0.5 则输出为 1）并预测类别值。 Logistic 回归 由于模型的学习方式，Logistic 回归的预测也可以作为给定数据实例（属于类别 0 或 1）的概率。这对于需要为预测提供更多依据的问题很有用。 像线性回归一样，Logistic 回归在删除与输出变量无关的属性以及非常相似（相关）的属性时效果更好。它是一个快速的学习模型，并且对于二分类问题非常有效。 线性判别分析（LDA）Logistic 回归是一种分类算法，传统上，它仅限于只有两类的分类问题。如果你有两个以上的类别，那么线性判别分析是首选的线性分类技术。 LDA 的表示非常简单直接。它由数据的统计属性构成，对每个类别进行计算。单个输入变量的 LDA 包括： 每个类别的平均值； 所有类别的方差。线性判别分析 进行预测的方法是计算每个类别的判别值并对具备最大值的类别进行预测。该技术假设数据呈高斯分布（钟形曲线），因此最好预先从数据中删除异常值。这是处理分类预测建模问题的一种简单而强大的方法。 分类与回归树决策树是预测建模机器学习的一种重要算法。 决策树模型的表示是一个二叉树。这是算法和数据结构中的二叉树，没什么特别的。每个节点代表一个单独的输入变量 x 和该变量上的一个分割点（假设变量是数字）。 决策树 决策树的叶节点包含一个用于预测的输出变量 y。通过遍历该树的分割点，直到到达一个叶节点并输出该节点的类别值就可以作出预测。 决策树学习速度和预测速度都很快。它们还可以解决大量问题，并且不需要对数据做特别准备。 朴素贝叶斯朴素贝叶斯是一个简单但是很强大的预测建模算法。 该模型由两种概率组成，这两种概率都可以直接从训练数据中计算出来：1）每个类别的概率；2）给定每个 x 的值，每个类别的条件概率。一旦计算出来，概率模型可用于使用贝叶斯定理对新数据进行预测。当你的数据是实值时，通常假设一个高斯分布（钟形曲线），这样你可以简单的估计这些概率。 贝叶斯定理 朴素贝叶斯之所以是朴素的，是因为它假设每个输入变量是独立的。这是一个强大的假设，真实的数据并非如此，但是，该技术在大量复杂问题上非常有用。 k近邻算法KNN 算法非常简单且有效。KNN 的模型表示是整个训练数据集。是不是很简单？ KNN 算法在整个训练集中搜索 K 个最相似实例（近邻）并汇总这 K 个实例的输出变量，以预测新数据点。对于回归问题，这可能是平均输出变量，对于分类问题，这可能是众数（或最常见的）类别值。 诀窍在于如何确定数据实例间的相似性。如果属性的度量单位相同（例如都是用英寸表示），那么最简单的技术是使用欧几里得距离，你可以根据每个输入变量之间的差值直接计算出来其数值。 K 近邻算法 KNN 需要大量内存或空间来存储所有数据，但是只有在需要预测时才执行计算（或学习）。你还可以随时更新和管理训练实例，以保持预测的准确性。 距离或紧密性的概念可能在非常高的维度（很多输入变量）中会瓦解，这对算法在你的问题上的性能产生负面影响。这被称为维数灾难。因此你最好只使用那些与预测输出变量最相关的输入变量。 学习向量量化K 近邻算法的一个缺点是你需要遍历整个训练数据集。学习向量量化算法（简称 LVQ）是一种人工神经网络算法，它允许你选择训练实例的数量，并精确地学习这些实例应该是什么样的。 学习向量量化 LVQ 的表示是码本向量的集合。这些是在开始时随机选择的，并逐渐调整以在学习算法的多次迭代中最好地总结训练数据集。在学习之后，码本向量可用于预测（类似 K 近邻算法）。最相似的近邻（最佳匹配的码本向量）通过计算每个码本向量和新数据实例之间的距离找到。然后返回最佳匹配单元的类别值或（回归中的实际值）作为预测。如果你重新调整数据，使其具有相同的范围（比如 0 到 1 之间），就可以获得最佳结果。 如果你发现 KNN 在你的数据集上达到很好的结果，请尝试用 LVQ 减少存储整个训练数据集的内存要求。 支持向量机（SVM）支持向量机可能是最受欢迎和最广泛讨论的机器学习算法之一。 超平面是分割输入变量空间的一条线。在 SVM 中，选择一条可以最好地根据输入变量类别（类别 0 或类别 1）对输入变量空间进行分割的超平面。在二维中，你可以将其视为一条线，我们假设所有的输入点都可以被这条线完全的分开。SVM 学习算法找到了可以让超平面对类别进行最佳分割的系数。 支持向量机 超平面和最近的数据点之间的距离被称为间隔。分开两个类别的最好的或最理想的超平面具备最大间隔。只有这些点与定义超平面和构建分类器有关。这些点被称为支持向量，它们支持或定义了超平面。实际上，优化算法用于寻找最大化间隔的系数的值。 SVM 可能是最强大的立即可用的分类器之一，值得一试。 Bagging 和随机森林随机森林是最流行和最强大的机器学习算法之一。它是 Bootstrap Aggregation（又称 bagging）集成机器学习算法的一种。 bootstrap 是从数据样本中估算数量的一种强大的统计方法。例如平均数。你从数据中抽取大量样本，计算平均值，然后平均所有的平均值以便更好的估计真实的平均值。 bagging 使用相同的方法，但是它估计整个统计模型，最常见的是决策树。在训练数据中抽取多个样本，然后对每个数据样本建模。当你需要对新数据进行预测时，每个模型都进行预测，并将所有的预测值平均以便更好的估计真实的输出值。 随机森林 随机森林是对这种方法的一种调整，在随机森林的方法中决策树被创建以便于通过引入随机性来进行次优分割，而不是选择最佳分割点。 因此，针对每个数据样本创建的模型将会与其他方式得到的有所不同，不过虽然方法独特且不同，它们仍然是准确的。结合它们的预测可以更好的估计真实的输出值。 如果你用方差较高的算法（如决策树）得到了很好的结果，那么通常可以通过 bagging 该算法来获得更好的结果。 Boosting 和 AdaBoostBoosting 是一种集成技术，它试图集成一些弱分类器来创建一个强分类器。这通过从训练数据中构建一个模型，然后创建第二个模型来尝试纠正第一个模型的错误来完成。一直添加模型直到能够完美预测训练集，或添加的模型数量已经达到最大数量。 AdaBoost 是第一个为二分类开发的真正成功的 boosting 算法。这是理解 boosting 的最佳起点。现代 boosting 方法建立在 AdaBoost 之上，最显著的是随机梯度提升。 AdaBoost AdaBoost 与短决策树一起使用。在第一个决策树创建之后，利用每个训练实例上树的性能来衡量下一个决策树应该对每个训练实例付出多少注意力。难以预测的训练数据被分配更多权重，而容易预测的数据分配的权重较少。依次创建模型，每个模型在训练实例上更新权重，影响序列中下一个决策树的学习。在所有决策树建立之后，对新数据进行预测，并且通过每个决策树在训练数据上的精确度评估其性能。 因为在纠正算法错误上投入了太多注意力，所以具备已删除异常值的干净数据非常重要。 总结初学者在面对各种机器学习算法时经常问：「我应该用哪个算法？」这个问题的答案取决于很多因素，包括：（1）数据的大小、质量和特性；（2）可用的计算时间；（3）任务的紧迫性；（4）你想用这些数据做什么。 即使是经验丰富的数据科学家在尝试不同的算法之前，也无法分辨哪种算法会表现最好。虽然还有很多其他的机器学习算法，但本篇文章中讨论的是最受欢迎的算法。如果你是机器学习的新手，这将是一个很好的学习起点。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer（四）：7-13]]></title>
    <url>%2Farchives%2F2bae0a51.html</url>
    <content type="text"><![CDATA[Fibonacci数列剑7-斐波那契数列大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。n&lt;=39123456789101112131415class Solution: def Fibonacci(self,n): if n == 0: return 0 if n == 1: return 1 if n == 2: return 1 if n &gt;= 3: s = []*n s.append(1) s.append(1) for i in range(2,n): s.append(s[i-1]+s[i-2]) return s[n-1] 剑8-跳台阶一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 分析： 1.假设当有n个台阶时假设有f(n)种走法。2.青蛙最后一步要么跨1个台阶要么跨2个台阶。3.当最后一步跨1个台阶时即之前有n-1个台阶，根据1的假设即n-1个台阶有f(n-1)种走法。 当最后一步跨2个台阶时即之前有n-2个台阶，根据1的假设即n-2个台阶有f(n-2 )种走法。5.显然n个台阶的走法等于前两种情况的走法之和即f(n)=f(n-1)+f(n-2)。6.找出递推公式后要找公式出口，即当n为1、2时的情况，显然n=1时f(1)等于1，f(2)等于2 | 1, (n=1)f(n) = |2, (n=2)| f(n-1)+f(n-2) ,(n&gt;2,n为整数) 12345678910111213141516class Solution: def jumpFloor(self, number): if number &lt;=0: return 0 elif number &lt;= 2: return number else: a = 1 b = 2 #a,b=1,2 for i in range(2,number): temp = a+b a = b b = temp #a,b = b,a+b return b 其中a,b=a,a+b是指：123t=aa=bb=b+t 也就是，此时a加的是未改变之前的a。举个例子：12345678910111213a，b=0,1 #复合赋值 实际上就是a=0,b=1while b&lt;10print(b)a,b=b,a+b #实际上还是复合赋值a=b,b=a+b;那输出结果将是a=0 b=1 1a=1 b=1 1a=1 b=2 2a=2 b=3 3a=3 b=5 5a=5 b=8 8a=8 b=13 #大于了10不做输出 剑9-变跳台阶一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 分析：因为n级台阶，第一步有n种跳法：跳1级、跳2级、到跳n级跳1级，剩下n-1级，则剩下跳法是f(n-1)跳2级，剩下n-2级，则剩下跳法是f(n-2)所以f(n)=f(n-1)+f(n-2)+…+f(1)因为f(n-1)=f(n-2)+f(n-3)+…+f(1)所以f(n)=2*f(n-1) 12345678class Solution: def jumpFloor2(self, number): if number &lt;= 0: return -1 elif number == 1: return 1 else: return 2*self.jumpFloor2(number-1) 剑10-矩形覆盖我们可以用21的小矩形横着或者竖着去覆盖更大的矩形。请问用n个21的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？ 分析： 逆向分析应为可以横着放或竖着放，多以f(n)可以是2(n-1)的矩形加一个竖着放的21的矩形或2*(n-2)的矩形加2横着放的，即f(n)=f(n-1)+f(n-2)当到了最后，f(1)=1,f(2)=2 1234567891011class Solution: def rectCover(self, number): if number &lt; 0: return -1 elif number &lt;= 2: return number else: a,b = 1,2 for i in range(2,number)： a,b = b,a+b return b 剑11-二进制中1的个数输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 //超级简单容易理解 //&amp;(与)// 把这个数逐次 右移 然后和1 与,//就得到最低位的情况,其他位都为0,//如果最低位是0和1与 之后依旧 是0，如果是1，与之后还是1。//对于32位的整数 这样移动32次 就记录了这个数二进制中1的个数了 123456class Solution: def NumberOf1(self,n): count = 0 for i in range(32): count += (n &gt;&gt; i)&amp;1 return count 剑12-数值整数次方给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。 123456789101112class Solution: def Power(self,base,exponent): if base == 0: return 0 elif base == 1: return 1 elif exponent == 0: return 1 elif exponent == 1: return base else: return pow(base,exponent) 剑13-调整数组顺序使奇数位于偶数前面输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。 1.要想保证原有次序，则只能顺次移动或相邻交换。 2.i从左向右遍历，找到第一个偶数。 3.j从i+1开始向后找，直到找到第一个奇数。 4.将[i,…,j-1]的元素整体后移一位，最后将找到的奇数放入i位置，然后i++。 5.終止條件：j向後遍歷查找失敗。12345678910class Solution: def reOrderArray(self,array): odd = [] even = [] for i in array: if i%2 == 1: odd.append(i) else: even.append(i) return odd+even]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>查找</tag>
        <tag>替换空格</tag>
        <tag>打印链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法（一）]]></title>
    <url>%2Farchives%2Fb04cb3d.html</url>
    <content type="text"><![CDATA[第一题 从1到500的500个数，第一次删除奇数位，第二次删除剩下来的奇数位，以此类推，最后剩下的唯一一位数是__ 分析思路：比如：1,2，删除奇数位，那剩下的是2，1,2,3，删除奇数位，剩下的是2，1,2,3,4,剩下的是4,1,2,3,4,5,6,7,剩下的是4,1,2,3,4,5,6,7,8和1,2,3,4,5,6,7,8,9,10,11,12,13,14,15剩下的是8，总结规律：当1~n，$2^i&lt;n&lt;2^(i+1)$时候，这样删除剩下的是$2^i$。$2^8&lt;500&lt;2^9$，所以剩下的就是$2^8=256$。 边缘算子 常用边缘检测有哪些算子，有什么特性？ Roberts，Sobel，Prewitt，Laplacian，Canny (1)Robert 优点：一种利用局部差分算子寻找边缘的算子，定位比较精确，但由于不包括平滑，所以对于噪声比较敏感。 缺点：对噪声敏感,无法抑制噪声影响。 (2)Sobel 优点：一阶微分算子，加权平均滤波，对低噪声图像有较好检测效果。 缺点：抗噪性差。 (3)Prewitt 优点：一阶微分算子，平均滤波，对低噪声图像有较好检测效果。 缺点：抗噪性差。 (4)Laplacian算子 优点：各向同性，二阶微分，精确定位边缘位置所在。 缺点：无法感知边缘强度。只适用于无噪声图象。存在噪声情况下，使用Laplacian算子检测边缘之前需要先进行低通滤波。 (5)Canny算子 是一个具有滤波，增强，检测的多阶段的优化算子。先利用高斯平滑滤波器来平滑图像以除去噪声，采用一阶偏导的有限差分来计算梯度幅值和方向，然后再进行非极大值抑制。 内存分配 C，C++程序编译的内存分配情况有哪几种，并简要解释。 C，C++程序编译的内存分配情况共有以下五种： (1)栈区（ stack ）:由编译器自动分配释放 ，存放为运行函数而分配的局部变量、函数参数、返回数据、返回地址等。其操作方式类似于数据结构中的栈。 (2)堆区（ heap ）:一般由程序员分配释放， 若程序员不释放，程序结束时可能由 OS 回收。分配方式类似于链表。 (3)全局区（静态区）（ static ）:存放全局变量、静态数据、常量。程序结束后有系统释放 (4)文字常量区: 常量字符串就是放在这里的。 程序结束后由系统释放。 (5)程序代码区:存放函数体（类成员函数和全局函数）的二进制代码。 cnn和fcn 简述CNN和FCN的区别 卷积层是CNN区别于其它类型神经网络的本质特点 不过CNN通常也不仅仅只包含卷积层，其也会包含全连接层，全连接层的坏处就在于其会破坏图像的空间结构，因此人们便开始用卷积层来“替代”全连接层，通常采用1 × 1的卷积核，这种不包含全连接层的CNN称为全卷积网络（FCN）。 FCN最初是用于图像分割任务，之后开始在计算机视觉领域的各种问题上得到应用，事实上，Faster R-CNN中用来生成候选窗口的CNN就是一个FCN。 编程题5（编程题）给出了一个n*n的矩形，编程求从左上角到右下角的路径数（n &gt; =2），并返回走的方法，限制只能向右或向下移动，不能回退。向右走此步骤标记为1， 向下走此步骤标记为2。12Int Solve(int b , std::vector&lt;std::vector&lt;int&gt;&gt;&amp; method) &#123;&#125; 算法分析：这是一个动态规划问题。因为限制只能向右或向下移动，所以当走到某一步时只有两种可能：一是从该处的上面走过来的；二是从该处的左边走过来的。假设我们到达某一处（i,j）, Path[i][j],可以得到Path[i][j] = Path[i - 1][j] + Path[i][j - 1].上述方程的边界条件出现在最左列（P [i] [j-1]不存在），最上一行（P [i-1] [j]不存在）。这些条件可以通过初始化（预处理）来处理 - 对于所有有效的i，j，初始化P [0] [j] = 1，P [i] [0] = 1。注意初始值是1而不是0。 不理解上面给的b和method，这里自己写了个solution：123456789class Solution &#123; int uniquePaths(int m, int n) &#123; vector&lt;vector&lt;int&gt; &gt; path(m, vector&lt;int&gt; (n, 1)); for (int i = 1; i &lt; m; i++) for (int j = 1; j &lt; n; j++) path[i][j] = path[i - 1][j] + path[i][j - 1]; return path[m - 1][n - 1]; &#125;&#125;; 6.（编程题）编程求解a＊x + b * sin x = 0。12Std::vector&lt;double&gt; Solve(double a, double b)&#123;&#125; 依然不知都写的啥，用matlab求解：123syms a b xsolve(a＊x + b * sin x == 0) &gt;&gt;ans.x 7、 （探索题）你会用什么样的方法提取遥感图像中的道路呢（包括主街道、田埂、小路）。示例如下：白色明显道路是大路，深绿色田埂也属于需要提取的区域。 （1） 对含中心线的城区主街道，采取基于模板匹配的半自动道路提取方法，可以实时准确的提取城区主干道路的中心线。以中心线上一点为中心建立对应模板窗，在沿道路前进方向寻找与之匹配的目标窗，将该目标窗中心作为下一个道路种子点，并以此生成新的模板窗，玄幻迭代即可得到一系列中心线上种子点，最后将其连成中心线。 （2） 对于田埂小路，采用基于像素匹配和基于比值直方图匹配的半自动提取算法，提取道路信息，当道路与周围环境对比度降低（周围大量的绿田），仍能保证较好结果。 8、 （随便聊聊题） 聊聊你认为深度学习中最有用的三个trick和三个创造性的idea，并说一下你的理解，为什么有用，为什么有创造性。 一．有用的三个trick （1）loss function: 用于衡量最优的策略。一般由一个损失项和一个正则化项组成。常见的损失函数有：0-1损失函数和绝对值损失函数，log对数损失函数，平方损失函数，指数损失函数 ，Hinge损失函数。 （2）kernel: 核函数是machine learning中最核心的部分的东西, 它有效的描述了点和点之间的关系，或者说是距离，当然这里的距离严格的说应该是广义的距离。当然核函数一般与曼哈顿距离，欧氏距离等以及空间（希尔伯特空间等）关联。 （3）activation function：激活函数不是真的要去激活什么。在神经网络中，激活函数的作用是能够给神经网络加入一些非线性因素，使得神经网络可以更好地解决较为复杂的问题。常用的三类激活函数有： A．Sigmoid函数的输出映射在(0,1)(0,1)之间，单调连续，输出范围有限，优化稳定，可以用作输出层。 B．tanh函数比Sigmoid函数收敛速度更快 C．ReLU比起Sigmoid和tanh能够快速收敛；Sigmoid和tanh涉及了很多很expensive的操作（比如指数），ReLU可以更加简单的实现；有效缓解了梯度消失的问题；在没有无监督预训练的时候也能有较好的表现；提供了神经网络的稀疏表达能力。 二．三个创造性的idea （1）GAN: 生成对抗神经网络。GAN已经被引入到了各种以往深度神经网络的任务中,例如从分割图像恢复原图像，给黑白图片上色，还可以做图像超分辨率,动态场景生成，图像去模糊等等。 （2）开源框架tensorflow: 不仅实现深度学习的可视化，实现了将深度学习算法移植到智能设备或手机应用中去。 （3）人脸识别，目标定位等一系列深度学习技术的成熟使人工智能走向工业智能。小米扫地机器人，物流运载机等等都可以算是深度学习的创造性成果。]]></content>
      <categories>
        <category>算法工程师面试</category>
      </categories>
      <tags>
        <tag>算法面试</tag>
        <tag>深度学习面试</tag>
        <tag>机器学习面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法：正则化]]></title>
    <url>%2Farchives%2Fca0f6c63.html</url>
    <content type="text"><![CDATA[前言正则化常和防止过拟合联系在一起，什么是过拟合，为什么会引起过拟合，怎么解决？过拟合产生的原因：模型过于复杂，参数太多什么是过拟合：模型训练时误差小，测试时误差大；也就是模型复杂到可以拟合我们所有的训练样本，但是在实际预测新样本的时候却一塌糊涂 简而言之，就是擅长背诵知识，不善于灵活应用；应试能力好，实际应用能力差。 通常过拟合由以下三种原因产生： 假设过于复杂； 数据存在很多噪音； 数据规模太小。 过拟合的解决方法通常有： early stopping； 数据集扩增； 正则化； Dropout。L0与L1-正则项（LASSO regularizer）在机器学习里，最简单的学习算法可能是所谓的线性回归模型 我们考虑这样一种普遍的情况，即：预测目标背后的真是规律，可能只和某几个维度的特征有关；而其它维度的特征，要不然作用非常小，要不然纯粹是噪声。在这种情况下，除了这几个维度的特征对应的参数之外，其它维度的参数应该为零。若不然，则当其它维度的特征存在噪音时，模型的行为会发生预期之外的变化，导致过拟合。 于是，我们得到了避免过拟合的第一个思路：使尽可能多的参数为零。为此，最直观地我们可以引入$L_0$-范数。令： 这意味着，我们希望绝大多数w⃗ 的分量为零。 通过引入 L0−正则项，我们实际上引入了一种「惩罚」机制，即：若要增加模型复杂度以加强模型的表达能力降低损失函数，则每次使得一个参数非零，则引入 ℓ0的惩罚系数。也就是说，如果使得一个参数非零得到的收益（损失函数上的收益）不足 ℓ0；那么增加这样的复杂度是得不偿失的。 通过引入L0−正则项，我们可以使模型稀疏化且易于解释，并且在某种意义上实现了「特征选择」。这看起来很美好，但是 L0−正则项也有绕不过去坎： 不连续 非凸 不可求导 因此，L0正则项虽好，但是求解这样的最优化问题，难以在多项式时间内找到有效解（NP-Hard 问题）。于是，我们转而考虑 L0-范数最紧的凸放松（tightest convex relaxation）：L1-范数。令： L2正则项（Ridge regularizer）让我们回过头，考虑多项式模型，它的一般形式为： 我们注意到，当多项式模型过拟合时，函数曲线倾向于靠近噪声点。 这意味着，函数曲线会在噪声点之间来回扭曲跳跃。 这也就是说，在某些局部，函数曲线的切线斜率会非常高（函数导数的绝对值非常大）。 对于多项式模型来说，函数导数的绝对值，实际上就是多项式系数的一个线性加和。 这也就是说，过拟合的多项式模型，它的参数的绝对值会非常大（至少某几个参数分量的绝对值非常大）。 因此，如果我们有办法使得这些参数的值，比较稠密均匀地集中在0附近，就能有效地避免过拟合。 于是我们引入L2−正则项，令 L1−正则项与L2−正则项的区别现在，我们考虑这样一个问题：为什么使用L1−正则项，会倾向于使得参数稀疏化；而使用L2−正则项，会倾向于使得参数稠密地接近于0？ 这里引用一张来自周志华老师的著作，《机器学习》（西瓜书）里的插图，尝试解释这个问题。 为了简便起见，我们只考虑模型有两个参数w1和w2的情形。 在图中，我们有三组等值线，位于同一条等值线上的w1与w2映射到相同的平方损失项、L1−范数和L2−范数。并且，对于三组等值线来说，当(w1,w2)(w1,w2)沿着等值线法线方向，向外扩张，则对应的值增大；反之，若沿着法线向内收缩，则对应的值减小。 因此，对于目标函数Obj(F)来说，实际上是要在正则项的等值线与损失函数的等值线中寻找一个交点，使得二者的和最小。 对于L1−正则项来说，因为L1−正则项是一组菱形，这些交点容易落在坐标轴上。因此，另一个参数的值在这个交点上就是0，从而实现了稀疏化。 对于 L2−正则项来说，因为 L2−正则项的等值线是一组圆形。所以，这些交点可能落在整个平面的任意位置。所以它不能实现「稀疏化」。但是，另一方面，由于 (w1,w2)(w1,w2) 落在圆上，所以它们的值会比较接近。这就是为什么 L2−正则项可以使得参数在零附近稠密而平滑。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer（三）：1-6]]></title>
    <url>%2Farchives%2F2853aed2.html</url>
    <content type="text"><![CDATA[二维数组中的查找在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 //思路//1：从前往后插入，这样移动·的次数多不建议//2：从后往前插 123456789101112131415161718class Solution:#array 2D-array def Find(self, target, array): # search in row for i in range(len(array)): # search in column for j in range(len(array[i])): # find target if target == array[i][j]: return 'true' return 'false'while True: try: S=Solution() target, array=input() print(S.Find(target,array)) except: break 以下这个是另外一种写法，也能够成功运行，但是这两种普遍蛮力1234567891011121314151617 def Find(self, target, array): for i in range(len(array)): for j in range(len(array[i])): if target == array[i][j]: return 'true' return 'false'flag = Truewhile flag: try: S=Solution() target = 1 array = [[1,2],[3,4]] print(S.Find(target,array)) flag = False except: break 算法思想： 由于数组从左到右，从上到下都是递增，可以从右上角或者左下角开始查找，这里从右上角开始查找 定义数组array，行i，列j，目标target。 若array[i][j]==target,那恭喜你找到目标； 若array[i][j]&gt;target,那向左去找目标–j； 否则，array[i][j]小于target,那向下寻找目标++i； 1234567891011121314class Solution: def Find(self, target, array): rows = len(array)-1 cols = len(array[i])-1 i = rows j = 0 while j &lt;= cols and i &gt;= 0 if target &lt; array[i][j]: i -= 1 else if target &gt; array[i][j]: j += 1 else return True return False 替换空格请实现一个函数，将一个字符串中的空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy 以下仍然是从前向后1234567891011121314151617181920212223242526272829# -*- coding:utf-8 -*-class Solution: # s 源字符串 def replaceSpace(self, s): return s.replace(' ', '%20')算法思想：1. 定义字符串下标i（i=0开始），定义空格bankNum,遍历字符串并计算空格个数；2. 总字符串=原始字符串下标+1+2*bankNum；3. 末尾字符串下标=totalNum-1，也就是我们说的放置位置；4. 总体来说：从后向前遍历字符串，如果遇到空格，就加%20；没有空格，就将原字符串的最后一个字符赋给新字符串```python# -*- coding:utf-8 -*-class Solution: # s 源字符串 def replaceSpace(self, s): # write code here str = [] num = s.count(' ') for char in s: if char == ' ' and num &gt; 0: char = '%20' num -= 1 str.append(char) newstr = ''.join(str) return newstr添加笔记 打印链表输入一个链表，从尾到头打印链表每个节点的值。 12345678class Solution: def printListFromTailToHead(self,listNode): l=[] head = listNode while head: l.insert(0,head.val) head=head.next return 1 二分查找1234567891011121314151617181920def search2(a,m): low=0 high=len(a)-1 while(low&lt;=high): mid=(low+high)/2 midval=a[mid] if midval&lt;m: low=mid+1 else if midval&gt;m: high=mid-1 else return mid return -1 if __name__=="__main__" a=[1,2,3,4,5,6,7,8,9] m=5 result=search2(a,m) print result 下面这个程序就会陷入无限循环while中123456789101112131415161718192021#include&lt;studio.h&gt;int bsearch(int a[],int n,int v)&#123; int left, middle, right; left=0, right=n-1; while(left&lt;=right) &#123; middle=left+(right-left)/2; if(a[middle]&gt;v) &#123; right=middle &#125; else if(a[middle]&lt;v) &#123; left=middle &#125; else return middle; &#125; return -1;&#125; 以下是正确写法123456789101112131415161718public static int binarySearch(int[] a,int n,int x)&#123; if(n&gt;0 &amp;&amp; x&gt;=a[0]) &#123; int left=0,right=n-1; while(left&lt;right) &#123; int middle=left+(right-left)/2; if(x&lt;a[middle]) right=middle-1; else left=middle; &#125; if(x==a[left]) return left; &#125; return -1;&#125; 重建二叉树输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。1234567891011121314151617# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回构造的TreeNode根节点 def reConstructBinaryTree(self, pre, tin): # write code here if len(pre) == 0: return None root = TreeNode(pre[0]) pos = tin.index(pre[0]) root.left = self.reConstructBinaryTree( pre[1:1+pos], tin[:pos]) root.right = self.reConstructBinaryTree( pre[pos+1:], tin[pos+1:]) return root 用两个栈实现队列用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型123456789101112class Solution: def __init__(self): self.stack1=[] self.stack2=[] def push(self,none): self.stack1.append(node) def pop(self): if self.stack2==[]: while self.stack1: self.stack2.append(self.stack1.pop()) return self.stack2.pop() return self.stack2.pop() 旋转数组中的最小数字把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。123456789101112class Solution: def minNumberInRotateArray(self, rotateArray): # write code here pre = -7e20 for num in rotateArray: if num &lt; pre : return num pre = num if len(rotateArray) == 0: return 0 return rotateArray[0]]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>查找</tag>
        <tag>替换空格</tag>
        <tag>打印链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法（三）]]></title>
    <url>%2Farchives%2F760c84b7.html</url>
    <content type="text"><![CDATA[简答题 小王在用svm做一个垃圾邮件分类器，如果一个邮件为垃圾邮件，则y=1，否则y=0。（1）小王应该一区那些特征？（10分）（2）在小王的训练集中，有99%都是非垃圾邮件，1%是垃圾邮件，如果最后训练的模型为对所有的邮件，都判定为非垃圾邮件，请问在训练集合中，准确率为多少？召回率为多少？（10分） (3)如果在应用场景中，希望能尽可能的召回垃圾邮件，怎么办？（10分） 算法与程序设计 给定一个长度为N的整数数组（元素有正有负），求所有元素之和最大的一个子数组，分析算法时间复杂度。（10） 百度每天接受用户的搜索查询，总是不停有搜索query进入日志，把query看作一个字符串，搜索日志就是一个字符串的数据流。这个字符串流永不停止。如何在这个不断增加的字符串流序列中，随机选择1个字符串？如果随机选择1000个字符串呢？（15） 在一个无限大平面上，有两组平行线，互相间垂直，每组平行线的间隔都为t，将一根长度为1（1小于t）的针任意掷在这个平面上，求此针与所有平行线都不想交的概率，采用蒙特卡洛方法，模拟计算这个概率值。（15）]]></content>
      <categories>
        <category>算法工程师面试</category>
      </categories>
      <tags>
        <tag>算法面试</tag>
        <tag>深度学习面试</tag>
        <tag>机器学习面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法（二）]]></title>
    <url>%2Farchives%2F3b1227e5.html</url>
    <content type="text"><![CDATA[#1. 基础算法题有序的数组，其中一个数出现一次，其中的数出现两次，找到这个出现一次的数？答：很自然想到hashmap或者异或的做法，时间复杂度O（N）。回答的时候觉得有坑，因为有序的条件没用，心想肯定跟二分有关系，果然面试官要求降低复杂度。没思考出来二分的策略，后来在面试官的提醒下明白了，二分的时候跟数组的位置联系起来，既数组肯定是基数个，二分的时候如果mid不成立的话，要找的值一定落在跟左右两边相等元素的那一侧，时间复杂度O（logN） #2.介绍项目聊天式文本挖掘，主要介绍了针对短文本的特征所做的特征工程 2.1.讲下LR模型答：在线性回归的基础上加了sigmoid函数，所有变成了分类模型，输出可以表示概率。采用似然估计构造目标函数，目标是优化最大似然估计，公式H(x)=-(ylogf(x)+(1-y)log(1-f(x)),一般优化方法采用的是梯度下降法。 2.2.LR模型为什么采用似然估计损失函数答：因为最小二乘法是假设残差服从正太分布的，而LR在sigmoid 作用后就不会服从正态分布了，所以采用的是最大似然估计。 后思考：1.最小二乘法反映的是线性空间上的线性投影的最短距离，在非线性空间上表现不如MLE。（MLE可以看作一种特殊情况下的Bayesian 估计，具体来说，就是在prior 是 diffuse （无知的）情况下，让posterior 分布取得极大值的系数值） 2.如果采用均方差最损失函数的时候，梯度下降求偏导时会有一项导数项，这样会导致梯度在一定阶段会收敛的特别慢，而对数损失函数log正好能和sigmoid的exp抵消掉，会加快收敛速度。 2.3.说下基本主题的LDA模型答：生成式模型， 思想：文档一定概率上从属于某些个主题，主题一定概率上会选中某些相关的词，这样就构造了文档到主题到词的联系，同时可以解决同义词问题，因为同义词可能属于不同主题。算法流程回答的模糊，感觉面试官不太满意。 2.4.说下项目用到的doc2vec怎么产生的？答：介绍了下word2vec的思想，然后讲传统上通常词向量简单的加权求和来表征一篇文档，而doc2vec训练方式是在word2vec的基础上，加入了段落ID，进行了一层训练，这样好处是保留了词的上下文信息。（ps：解释的不清楚，自己不太满意） 2.5.说下论文中频繁序列挖掘prefixspan 算法？答：（ps：因为这个算法不做序列挖掘基本不知道，可能只了解apriori算法）对比apriori算法的过程和缺点，讲解该算法的优势，只需要扫描一次序列数据集，目标是挖掘出满足最小支持度的频繁序列，长度为1的前缀开始挖掘序列模式，搜索对应的投影数据库的频繁序列，然后递归的挖掘长度为2的前缀所对应的频繁序列。以此类推，一直递归到对应的投影数据库为空或者对应投影数据库中各项的支持度计数小于阈值为止。整个过程就是前缀不断的增长，产生1，2…N 频繁序列，对应的投影数据库不断缩小直至为空。 优点：PrefixSpan算法由于不用产生候选序列，且投影数据库缩小的很快，内存消耗比较稳定，作频繁序列模式挖掘的时候效果很高。 3.了解深度学习吗？能否讲下CNN的特点？答：神经网络模型前向传递，反向调节的特点（BP网络）,隐含层我觉得是一个特征做变换的过程，整个过程给人的感觉就是向前是特征拓展阶段，向后是参数调优阶段。 回到CNN，特点是局部感受和权值共享，通过卷积核扫描原始数据能够学习到不同的局部的特征，接着通过池化进一步提取特征，这些做的能够让参数数目有量级的减少，同时权值共享是同一层隐含层共享权值，这样也是减少了隐含层的参数，很多卷积核学习的到特征最后传递到下一层网络，到最后一层采用分类器分类（扯不下去了，开始瞎扯）。 深度学习解决了以往神经网络深度网络很多问题，梯度消失爆炸问题，几个方面： 一是激活函数不光是只用sigmoid函数，还有 ReLU函数 二是在参数并不是初始化的时候并不是随机选择的，而是在前面有自编码器做了特征特征器，这样避免了梯度下降法求解陷入局部最优解； 三，深度学习一些手段，权值共享，卷积核，pooling等都能抑制梯度消失问题； 四，二次代价函数换成交叉熵损失函数或者选用softmax+对数似然代价函数的组合。4.说说RBM编码器答：）一种特征探测器，每一层学习的特征向上传递，然后反过来微调。好吧，只能回答这么多了。 补充：RBM包括隐层，可见层和偏置层。可见层和隐含层可以双向传播。标准的RBM，隐含层和可见层都是二进制表示，既激活函数的激活值服从二项分布。每一层的节点没有链接，如果假设所有的节点都只能取0或者1，同时全概率分布p(v,h)满足伯努利分布。 几个参数：一可视层和隐含层的权重矩阵 二是可是节点的偏移量 三是隐层的偏移量。这几个参数决定将N维的样本编码M维的样本。 用途：1.降维，类似稀疏自动编码器2.用RBM训练得到的权重举证和偏移量作为BP神经网路的初始值，避免陷入局部极小值3.可以估计联合分布P(v,h)，进而求出p（h|v）。生成式模型4.直接计算p（h|v）进行分类。判别式模型]]></content>
      <categories>
        <category>算法工程师面试</category>
      </categories>
      <tags>
        <tag>算法面试</tag>
        <tag>深度学习面试</tag>
        <tag>机器学习面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[别人的算法题]]></title>
    <url>%2Farchives%2F9c49b8cc.html</url>
    <content type="text"><![CDATA[#1. 基础算法题有序的数组，其中一个数出现一次，其中的数出现两次，找到这个出现一次的数？答：很自然想到hashmap或者异或的做法，时间复杂度O（N）。回答的时候觉得有坑，因为有序的条件没用，心想肯定跟二分有关系，果然面试官要求降低复杂度。没思考出来二分的策略，后来在面试官的提醒下明白了，二分的时候跟数组的位置联系起来，既数组肯定是基数个，二分的时候如果mid不成立的话，要找的值一定落在跟左右两边相等元素的那一侧，时间复杂度O（logN） #2.介绍项目聊天式文本挖掘，主要介绍了针对短文本的特征所做的特征工程 2.1.讲下LR模型答：在线性回归的基础上加了sigmoid函数，所有变成了分类模型，输出可以表示概率。采用似然估计构造目标函数，目标是优化最大似然估计，公式H(x)=-(ylogf(x)+(1-y)log(1-f(x)),一般优化方法采用的是梯度下降法。 2.2.LR模型为什么采用似然估计损失函数答：因为最小二乘法是假设残差服从正太分布的，而LR在sigmoid 作用后就不会服从正态分布了，所以采用的是最大似然估计。 后思考：1.最小二乘法反映的是线性空间上的线性投影的最短距离，在非线性空间上表现不如MLE。（MLE可以看作一种特殊情况下的Bayesian 估计，具体来说，就是在prior 是 diffuse （无知的）情况下，让posterior 分布取得极大值的系数值） 2.如果采用均方差最损失函数的时候，梯度下降求偏导时会有一项导数项，这样会导致梯度在一定阶段会收敛的特别慢，而对数损失函数log正好能和sigmoid的exp抵消掉，会加快收敛速度。 2.3.说下基本主题的LDA模型答：生成式模型， 思想：文档一定概率上从属于某些个主题，主题一定概率上会选中某些相关的词，这样就构造了文档到主题到词的联系，同时可以解决同义词问题，因为同义词可能属于不同主题。算法流程回答的模糊，感觉面试官不太满意。 2.4.说下项目用到的doc2vec怎么产生的？答：介绍了下word2vec的思想，然后讲传统上通常词向量简单的加权求和来表征一篇文档，而doc2vec训练方式是在word2vec的基础上，加入了段落ID，进行了一层训练，这样好处是保留了词的上下文信息。（ps：解释的不清楚，自己不太满意） 2.5.说下论文中频繁序列挖掘prefixspan 算法？答：（ps：因为这个算法不做序列挖掘基本不知道，可能只了解apriori算法）对比apriori算法的过程和缺点，讲解该算法的优势，只需要扫描一次序列数据集，目标是挖掘出满足最小支持度的频繁序列，长度为1的前缀开始挖掘序列模式，搜索对应的投影数据库的频繁序列，然后递归的挖掘长度为2的前缀所对应的频繁序列。以此类推，一直递归到对应的投影数据库为空或者对应投影数据库中各项的支持度计数小于阈值为止。整个过程就是前缀不断的增长，产生1，2…N 频繁序列，对应的投影数据库不断缩小直至为空。 优点：PrefixSpan算法由于不用产生候选序列，且投影数据库缩小的很快，内存消耗比较稳定，作频繁序列模式挖掘的时候效果很高。 3.了解深度学习吗？能否讲下CNN的特点？答：神经网络模型前向传递，反向调节的特点（BP网络）,隐含层我觉得是一个特征做变换的过程，整个过程给人的感觉就是向前是特征拓展阶段，向后是参数调优阶段。 回到CNN，特点是局部感受和权值共享，通过卷积核扫描原始数据能够学习到不同的局部的特征，接着通过池化进一步提取特征，这些做的能够让参数数目有量级的减少，同时权值共享是同一层隐含层共享权值，这样也是减少了隐含层的参数，很多卷积核学习的到特征最后传递到下一层网络，到最后一层采用分类器分类（扯不下去了，开始瞎扯）。 深度学习解决了以往神经网络深度网络很多问题，梯度消失爆炸问题，几个方面： 一是激活函数不光是只用sigmoid函数，还有 ReLU函数 二是在参数并不是初始化的时候并不是随机选择的，而是在前面有自编码器做了特征特征器，这样避免了梯度下降法求解陷入局部最优解； 三，深度学习一些手段，权值共享，卷积核，pooling等都能抑制梯度消失问题； 四，二次代价函数换成交叉熵损失函数或者选用softmax+对数似然代价函数的组合。4.说说RBM编码器答：）一种特征探测器，每一层学习的特征向上传递，然后反过来微调。好吧，只能回答这么多了。 补充：RBM包括隐层，可见层和偏置层。可见层和隐含层可以双向传播。标准的RBM，隐含层和可见层都是二进制表示，既激活函数的激活值服从二项分布。每一层的节点没有链接，如果假设所有的节点都只能取0或者1，同时全概率分布p(v,h)满足伯努利分布。 几个参数：一可视层和隐含层的权重矩阵 二是可是节点的偏移量 三是隐层的偏移量。这几个参数决定将N维的样本编码M维的样本。 用途：1.降维，类似稀疏自动编码器2.用RBM训练得到的权重举证和偏移量作为BP神经网路的初始值，避免陷入局部极小值3.可以估计联合分布P(v,h)，进而求出p（h|v）。生成式模型4.直接计算p（h|v）进行分类。判别式模型]]></content>
      <categories>
        <category>算法工程师面试</category>
      </categories>
      <tags>
        <tag>算法面试</tag>
        <tag>深度学习面试</tag>
        <tag>机器学习面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法学习（六）：SVM]]></title>
    <url>%2Farchives%2Fdf094319.html</url>
    <content type="text"><![CDATA[适用于初学SVM 了解SVM支持向量机，因其英文名为support vector machine，故一般简称SVM，通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。 1. 分类标准的起源：Logistic回归理解SVM，咱们必须先弄清楚一个概念：线性分类器。 给定一些数据点，它们分别属于两个不同的类，现在要找到一个线性分类器把这些数据分成两类。如果用x表示数据点，用y表示类别（y可以取1或者-1，分别代表两个不同的类），一个线性分类器的学习目标便是要在n维的数据空间中找到一个超平面（hyperplane），这个超平面的方程可以表示为（ wT中的T代表转置）$$w^Tx+ b = 0$$可能有读者对类别取1或-1有疑问，事实上，这个1或-1的分类标准起源于logistic回归。$$h_\theta (x) = g(\theta^Tx) = \frac{1}{1+e^-\theta^Tx} $$ 其中x是n维特征向量，函数g就是logistic函数。 $$g(z) = \frac{1}{1+e^-z}$$的图像是： 可以看到，上图将无穷映射到了(0,1)。 假设函数就是特征属于y=1的概率。$$P(y=1|x; \theta) = h\theta(x)$$$$P(y=0|x; \theta) = 1-h\theta(x)$$ 从而，当我们要判别一个新来的特征属于哪个类时，只需求即可，若大于0.5就是y=1的类，反之属于y=0类。 此外，$h\theta(x)$只和$\theta^Tx$有关，$\theta^Tx&gt;0$，那么$h\theta(x)&gt;0.5$，而g(z)只是用来映射，真实的类别决定权还是在于$\theta^Tx$。再者，当$\theta^Tx&gt;&gt;0$时，$h\theta(x)=1$，反之$h\theta(x)=0$。如果我们只从$\theta^Tx$出发，希望模型达到的目标就是让训练数据中y=1的特征t$\theta^Tx&gt;&gt;0$，而是y=0的特征。Logistic回归就是要学习得到$\theta$，使得正例的特征远大于0，负例的特征远小于0，而且要在全部训练实例上达到这个目标。 接下来，尝试把logistic回归做个变形。首先，将使用的结果标签y = 0和y = 1替换为y = -1,y = 1，然后将$\theta^Tx=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n(x_0=1)$中的$\theta_0$替换为b，最后将后面的$\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$替换为$\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nxn$（即$w^Tx$）。如此，则有了$\theta^Tx=w^Tx+ b$。也就是说除了y由y=0变为y=-1外，线性分类函数跟logistic回归的形式化表示$h\theta (x) = g(\theta^Tx) =g(w^Tx+b)$没区别。 进一步，可以将假设函数中的g(z)做一个简化，将其简单映射到y=-1和y=1上。映射关系如下： 2. 线性分类的一个例子下面举个简单的例子。如下图所示，现在有一个二维平面，平面上有两种不同的数据，分别用圈和叉表示。由于这些数据是线性可分的，所以可以用一条直线将这两类数据分开，这条直线就相当于一个超平面，超平面一边的数据点所对应的y全是-1 ，另一边所对应的y全是1。 这个超平面可以用分类函数表示，当f(x) 等于0的时候，x便是位于超平面上的点，而f(x)大于0的点对应 y=1 的数据点，f(x)小于0的点对应y=-1的点，如下图所示： 注：有的资料上定义特征到结果的输出函数，与这里定义的$w^Tx+ b$实质是一样的。为什么？因为无论是哪个，不影响最终优化结果。下文你将看到，当我们转化到优化 的时候，为了求解方便，会把yf(x)令为1，即yf(x)是y(w^x + b)，还是y(w^x - b)，对我们要优化的式子max1/||w||已无影响。（有一朋友飞狗来自Mare_Desiderii，看了上面的定义之后，问道：请教一下SVM functional margin 为=y(wTx+b)=yf(x)中的Y是只取1和-1 吗？y的唯一作用就是确保functional margin的非负性？真是这样的么？当然不是，详情请见本文评论下第43楼） 换言之，在进行分类的时候，遇到一个新的数据点x，将x代入f(x) 中，如果f(x)小于0则将x的类别赋为-1，如果f(x)大于0则将x的类别赋为1。 接下来的问题是，如何确定这个超平面呢？从直观上而言，这个超平面应该是最适合分开两类数据的直线。而判定“最适合”的标准就是这条直线离直线两边的数据的间隔最大。所以，得寻找有着最大间隔的超平面。 3. 函数间隔function margin和集合间隔geometrical margin在超平面wx+b=0确定的情况下，|wx+b|能够表示点x到距离超平面的远近，而通过观察wx+b的符号与类标记y的符号是否一致可判断分类是否正确，所以，可以用(y(w*x+b))的正负性来判定或表示分类的正确性。于此，我们便引出了函数间隔（functional margin）的概念。 定义函数间隔（用表示）为：而超平面(w，b)关于T中所有样本点(xi，yi)的函数间隔最小值（其中，x是特征，y是结果标签，i表示第i个样本），便为超平面(w, b)关于训练数据集T的函数间隔： 但这样定义的函数间隔有问题，即如果成比例的改变w和b（如将它们改成2w和2b），则函数间隔的值f(x)却变成了原来的2倍（虽然此时超平面没有改变），所以只有函数间隔还远远不够。 事实上，我们可以对法向量w加些约束条件，从而引出真正定义点到超平面的距离–几何间隔（geometrical margin）的概念。 假定对于一个点 x ，令其垂直投影到超平面上的对应点为 x0 ，w 是垂直于超平面的一个向量，为样本x到超平面的距离，如下图所示 根据平面几何知识，有 其中||w||为w的二阶范数（范数是一个类似于模的表示长度的概念），是单位向量（一个向量除以它的模称之为单位向量）。 又由于 x0 是超平面上的点，满足 f(x0)=0 ，代入超平面的方程$w^Tx+ b = 0$，可得$w^Tx_0+ b = 0$，即$w^Tx_0= -b$。 随即让此式的两边同时乘以$w^T$，再根据$w^Tx_0= -b$和$w^Tw= ||w||^2$，即可算出： 为了得到r的绝对值，令r乘上对应的类别 y，即可得出几何间隔（用表示）的定义： 从上述函数间隔和几何间隔的定义可以看出：几何间隔就是函数间隔除以||w||，而且函数间隔y(wx+b) = yf(x)实际上就是|f(x)|，只是人为定义的一个间隔度量，而几何间隔|f(x)|/||w||才是直观上的点到超平面的距离。 4. 最大间隔分类器Maximum Margin Classifier的定义对一个数据点进行分类，当超平面离数据点的“间隔”越大，分类的确信度（confidence）也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化这个“间隔”值。这个间隔就是下图中的Gap的一半。 通过由前面的分析可知：函数间隔不适合用来最大化间隔值，因为在超平面固定以后，可以等比例地缩放w的长度和b的值，这样可以使得$f(x)=w^Tx+ b$的值任意大，亦即函数间隔可以在超平面保持不变的情况下被取得任意大。但几何间隔因为除上了||w||，使得在缩放w和b的时候几何间隔的值是不会改变的，它只随着超平面的变动而变动，因此，这是更加合适的一个间隔。换言之，这里要找的最大间隔分类超平面中的“间隔”指的是几何间隔。 于是最大间隔分类器（maximum margin classifier）的目标函数可以定义为：$$max\hat{\gamma}$$ 同时需满足一些条件，根据间隔的定义，有 其中，s.t.，即subject to的意思，它导出的是约束条件。 回顾下几何间隔的定义 可知：如果令函数间隔等于1（之所以令等于1，是为了方便推导和优化，且这样做对目标函数的优化没有影响，至于为什么，请见本文评论下第42楼回复），则有$\gamma = 1 / ||w||$且，从而上述目标函数转化成了 相当于在相应的约束条件下，最大化这个1/||w||值，而1/||w||便是几何间隔$\hat{\gamma}$。 如下图所示，中间的实线便是寻找到的最优超平面（Optimal Hyper Plane），其到两条虚线边界的距离相等，这个距离便是几何间隔$\hat{\gamma}$，两条虚线间隔边界之间的距离等于2$\hat{\gamma}$，而虚线间隔边界上的点则是支持向量。由于这些支持向量刚好在虚线间隔边界上，所以它们满足（还记得我们把 functional margin 定为 1 了吗？上节中：处于方便推导和优化的目的，我们可以令$\hat{\gamma}$=1），而对于所有不是支持向量的点，则显然有。 OK，到此为止，算是了解到了SVM的第一层，对于那些只关心怎么用SVM的朋友便已足够，不必再更进一层深究其更深的原理。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>算法</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构（二）]]></title>
    <url>%2Farchives%2F266a3580.html</url>
    <content type="text"><![CDATA[线性数据结构例：以下数据结构中，（）是非线性数据结构A.树B.字符串C.队D.栈 线性结构 是一个有序数据元素的集合。 a. 其中数据元素之间的关系是一对一的关系，即除了第一个和最后一个数据元素之外，其它数据元素都是首尾相接的。 b. 常用的线性结构有：线性表，栈，队列，双队列，数组，串 非线性结构 中各个数据元素不再保持在一个线性序列中，每个数据元素可能与零个或者多个其他数据元素发生联系。 a. 根据关系的不同，可分为层次结构和群结构。 b. 常见的非线性结构有：二维数组，多维数组，广义表，树(二叉树等)，图 字符串输出有字符数组 a[80] 和 b[80]，则正确的输出语句是（）A.puts(a); puts(b);B.printf(“%s,%s”, a[], b[]);C.putchar(a, b);D.puts(a, b); puts（）输出一个字符串，遇到’\0’结束，putchar（）输出单个字符 puts()函数用来向标准输出设备（屏幕）写字符串并换行，其调用方式为，puts(s)；其中s为字符串字符（字符串数组名或字符串指针）。 用法：int puts(const char *string); 则程序的输出结果是: H 91234567main( ) &#123; char c1,c2; c1 ='C'+'8'－'3'; c2 ='9'－'0'; printf("%c %d\n",c1,c2);&#125; 分析： string 和 int 型都支持直接加减 ‘C’+’8’-‘3’= ‘C’+’5’,由于’C’+1=’D’，所以结果为char ‘H’ =&gt; %c； ‘9’-‘0’：平时写代码的时候经常int（0~9）转换char就用的+’0’,因此结果直接就是int 9 =&gt; %d。 本题考查字符变量以及printf( )函数相关知识,字符变量c1被赋值为’C’+’8’－’3’,即ASSCII码的运算,67十54－49=72,即H;字符变量 c2被赋值为’9’－’0’,但输出时,需要注意的是c1以字符变量输出,而c2是以十进制整型变量输出。 栈设栈的初始状态为空，当字符序列a3_作为栈的输入时，输出长度为3的且可以用作C语言标识符的字符串序列有（3）个。 首先，栈的顺序是先进后出字符序列为a3 1)a入栈，再出栈，然后3入栈，再出栈，—入栈，再出栈 序列是a3 2)a入栈，再出栈,然后3,—入栈，再出栈，序列是a3 3)a入栈，3入栈，再出栈，a出栈， —入栈，再出栈 序列是3a 4) a入栈，3入栈，再出栈, —入栈,序列是3a 5) a入栈，3入栈,入栈，序列是3a其次，C语言的标识符不能以数字开头，去除3a和3_a 答案为3 补充一下卡特兰数公式 h(n)=C(2n，n)/(n+1)，适用于出栈情况求和.】 根据卡特兰数公式，所有的输出总数为5次，减掉3开头的2个为3个。 栈和队栈：先进后出；栈只能在栈顶插入和删除数据。 栈的应用： 1、符号匹配； 2、表达式求值； 3、实现函数调用 4、栈是解决封闭对应问题的有效方法。 栈是限定在一端进行插入与删除的线性表，允许插入与删除的一端称为栈顶，不允许插入与删除的另一端称为栈底。栈按照“先进后出”(FILO)或“后进先出”(LIFO)组织数据，栈具有记忆作用 队：先进先出；队列只能在队尾插入数据，在队首删除数据 在递归算法执行过程中，计算机系统必定会用到的数据结构是（ 栈） 递归的过程，利用栈保存现场地址，然后将数据入栈，运算，后出栈，返回结果。 中缀表达式已知-算术表达式的中缀表达式为a-(b+c/d)e,其后缀形式为()A. -a+bc/dB.-a+bcd/eC.-+abc/deD.abcd/+e*- 后缀表达式不包含括号，运算符 放在两个运算对象的后面，所有的计算按运算符出现的顺序，严格从左向右进行（不再考虑运算符的优先规则）先是c/d写为cd/，(b+c/d)写为bcd/+，(b+c/d)e写为bcd/+e,a- ( b+c/d)e写为abcd/+e-。 这里我给出一个中缀表达式：a+bc-(d+e)第一步：按照运算符的优先级对所有的运算单位加括号：式子变成了：((a+(bc))-(d+e))第二步：转换前缀与后缀表达式前缀：把运算符号移动到对应的括号前面则变成了：-( +(a (bc)) +(de))把括号去掉：-+abc+de 前缀式子出现后缀：把运算符号移动到对应的括号后面则变成了：((a(bc) )+ (de)+ )-把括号去掉：abc+de+- 后缀式子出现 中序遍历，运算符一定是父节点。 sql若要删除 book 表中的所有数据，如下哪些语法是错误的？A.drop table book;B.truncate table book;C.delete from book;D.delelet *from book; A： drop table book 是删除整个表，题目的潜在意思是删除表中的数据而并非删除整个表。因此A错。B： truncate table book 是删除表中的数据，删除速度比delete更快，无法撤回（回退）。C： delete from book 删除数据表中的数据，可以回退，可添加where 子句。D：语法错误。]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>堆</tag>
        <tag>栈</tag>
        <tag>图</tag>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字符串]]></title>
    <url>%2Farchives%2Ffc81fbfd.html</url>
    <content type="text"><![CDATA[字符串例：下面 是‘ abcd321ABCD ’的子串。DA.abcdB.321abC.‘abc ABC’D.‘21AB’ 串中任意个连续的字符组成的子序列称为该串的子串。 空串和本身都算做本字符串的字串。 将两个字符串连接起来组成一个字符串时，选用（ ）函数。A.strlen()B.strcap()C.strcat()D.strcmp() strcat()字符串连接字符 strcmp()字符串大小比较 赋值不能把字符串”HELLO!”赋给数组b的语句是（）A.char b[10]={‘H’，’E’，’L’，’L’，’O’，’!’，’\0’};B.char b[10];b=”HELLO!”;C.char b[10];strcpy(b，”HELLO!”);D.char b[10]=”HELLO!”; 字符数组初始化有两种方法：一种是逐个字符赋值，另一种是用字符常量对整个数组赋值。 A是第一种，D是第二种。显然第一种比第二种繁琐复杂。 选项B并没有将数组b赋值为 hello！ 因子b是数组的首地址，b=”HELLO!”;是改变了这个指针的指向，是错误的。 C是字符串拷贝函数。函数格式： char strcpy (char s1, const char *s2);功能： 将S2所指的字符串拷贝到S1所指的字符串中。说明：（1）参数S1S2都是指向字符串的指针。S1可以是字符数组名或字符指针，但不能是字符型常量，S2可以是字符串常量、字符数组或字符指针。（2）将S2所指的字符串拷贝到S1所指的字符串中，用赋值语句S1=S2;是不行的，赋值语句要求左边是左值，S1是常量。 （3）要保证S1的长度足够大，以便能容纳下S2所指的字符串，否则引起错误。 C 对于非strtic型数组不初始化，其元素值不能确定。对strtic数组元素不赋初值，系统会自动赋以0值。（参考） 安全值？判断下述语句的对错：MFC中CString是类型安全的类。（对） 类型安全就是说，如果两个类型直接要相互转换，必须要显示的转换，不能偷偷摸摸的只用一个等于号就隐式转换了。 MFC数据类型转换标准库std的string 和MFC类库CString之间可以通过CString的format方法进行转换 类型安全不是一种类型，是有关类型操作一种规范。 如：不让不同类型的数据相互转换int Num =3;string Str=”3”;Num =Str; //错Num=int.Parse(Str);//对类型安全要求可以相互转换的不同类型数据在转换时 显式转换 字符串倒序请找出下面代码中的所有错误。说明：以下代码是把一个字符串倒序，如“abcd”倒序后变为“dcba”。 123456789101112131415#include "string.h" int main() &#123; char *src = "hello,world"; char *dest = NULL; int len = strlen(src); dest = (char *)malloc(len); char *d = dest; char *s = src[len]; while (len-- != 0) *d++ = *s--; printf("%s", dest); return 0; &#125; 第7行要为’\0’分配一个空间 第9行改成char * s = &amp;src[len-1] 第12行前要加上*d = ‘\0’ 第13行前要加上free(dest)释放空间 方法一：1234567891011121314int main()&#123; char *src = "hello,world"; int len = strlen(src); char *dest = (char *)malloc(len + 1); //要为\0分配一个空间 char *d = dest; char *s = &amp;src[len - 1]; //指向最后一个字符 while ( len-- != 0 ) *d++ = *s--; *d = 0; //尾部要加\0 printf("%s\n", dest); free(dest);// 使用完，应当释放空间，以免造成内存汇泄露 return 0;&#125; 方法二1234567891011121314int main()&#123; char str[] = "hello,world"; int len = strlen(str); char t; for (int i = 0; i &lt; len; i++) &#123; t = str[i]; str[i] = str[len - i - 1]; str[len - i - 1] = t; &#125; printf("%s", str); return 0;&#125;]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>堆</tag>
        <tag>栈</tag>
        <tag>图</tag>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构（四）]]></title>
    <url>%2Farchives%2Fdc598fb7.html</url>
    <content type="text"><![CDATA[字符串例：下面 是‘ abcd321ABCD ’的子串。DA.abcdB.321abC.‘abc ABC’D.‘21AB’ 串中任意个连续的字符组成的子序列称为该串的子串。 空串和本身都算做本字符串的字串。 将两个字符串连接起来组成一个字符串时，选用（ ）函数。A.strlen()B.strcap()C.strcat()D.strcmp() strcat()字符串连接字符 strcmp()字符串大小比较 赋值不能把字符串”HELLO!”赋给数组b的语句是（）A.char b[10]={‘H’，’E’，’L’，’L’，’O’，’!’，’\0’};B.char b[10];b=”HELLO!”;C.char b[10];strcpy(b，”HELLO!”);D.char b[10]=”HELLO!”; 字符数组初始化有两种方法：一种是逐个字符赋值，另一种是用字符常量对整个数组赋值。 A是第一种，D是第二种。显然第一种比第二种繁琐复杂。 选项B并没有将数组b赋值为 hello！ 因子b是数组的首地址，b=”HELLO!”;是改变了这个指针的指向，是错误的。 C是字符串拷贝函数。函数格式： char strcpy (char s1, const char *s2);功能： 将S2所指的字符串拷贝到S1所指的字符串中。说明：（1）参数S1S2都是指向字符串的指针。S1可以是字符数组名或字符指针，但不能是字符型常量，S2可以是字符串常量、字符数组或字符指针。（2）将S2所指的字符串拷贝到S1所指的字符串中，用赋值语句S1=S2;是不行的，赋值语句要求左边是左值，S1是常量。 （3）要保证S1的长度足够大，以便能容纳下S2所指的字符串，否则引起错误。 C 对于非strtic型数组不初始化，其元素值不能确定。对strtic数组元素不赋初值，系统会自动赋以0值。（参考） 安全值？判断下述语句的对错：MFC中CString是类型安全的类。（对） 类型安全就是说，如果两个类型直接要相互转换，必须要显示的转换，不能偷偷摸摸的只用一个等于号就隐式转换了。 MFC数据类型转换标准库std的string 和MFC类库CString之间可以通过CString的format方法进行转换 类型安全不是一种类型，是有关类型操作一种规范。 如：不让不同类型的数据相互转换int Num =3;string Str=”3”;Num =Str; //错Num=int.Parse(Str);//对类型安全要求可以相互转换的不同类型数据在转换时 显式转换 字符串倒序请找出下面代码中的所有错误。说明：以下代码是把一个字符串倒序，如“abcd”倒序后变为“dcba”。 123456789101112131415#include "string.h" int main() &#123; char *src = "hello,world"; char *dest = NULL; int len = strlen(src); dest = (char *)malloc(len); char *d = dest; char *s = src[len]; while (len-- != 0) *d++ = *s--; printf("%s", dest); return 0; &#125; 第7行要为’\0’分配一个空间 第9行改成char * s = &amp;src[len-1] 第12行前要加上*d = ‘\0’ 第13行前要加上free(dest)释放空间 方法一：1234567891011121314int main()&#123; char *src = "hello,world"; int len = strlen(src); char *dest = (char *)malloc(len + 1); //要为\0分配一个空间 char *d = dest; char *s = &amp;src[len - 1]; //指向最后一个字符 while ( len-- != 0 ) *d++ = *s--; *d = 0; //尾部要加\0 printf("%s\n", dest); free(dest);// 使用完，应当释放空间，以免造成内存汇泄露 return 0;&#125; 方法二1234567891011121314int main()&#123; char str[] = "hello,world"; int len = strlen(str); char t; for (int i = 0; i &lt; len; i++) &#123; t = str[i]; str[i] = str[len - i - 1]; str[len - i - 1] = t; &#125; printf("%s", str); return 0;&#125;]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>堆</tag>
        <tag>栈</tag>
        <tag>图</tag>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构（三）]]></title>
    <url>%2Farchives%2F6b7496d2.html</url>
    <content type="text"><![CDATA[折半查找与顺序查找两者查找速度比较：例如在一个数组中有10个元素. 第一个是要找的元素.折半查找：先找第六（下标为5）个,再找第三个（下标为2）,然后是第二个（下标为1）,最后是第一个（下标为0）…顺序查找：只要找一次就ok了. 第10个是要找的元素.折半查找：先找第六（下标为5）个,再找第八个（下标为7）,然后是第九个（下标为8）,最后是第十个（下标为9）…顺序查找：需要10次. 第三个是要找的元素.折半查找：先找第六（下标为5）个,再找第三个（下标为2）顺序查找：需要三次（效率一样）. 折半查找用向量和单链表示的有序表均可使用折半查找方法来提高查找速度(错) 折半查找属于随机访问特性 链表不行 堆排序也不能用链表 因为调整堆时没法随机访问底层孩子节点 快速排序可以链表 归并排序可用链表 基数排序可用链表 插入排序链表比数组要快一些 减少移动次数 具有12个关键字的有序表,折半查找的平均查找长度() 将12个数画成完全二叉树，第一层有1个、第二次2个、第三层4个，第四层只有5个。二分查找时：第一层需要比较1次第二两个数，每个比较2次第三层四个数，每个比较3次第四层五个数，每个比较4次则平均查找长度即为：（1+22+34+4*5）/12 = 37/12 = 3.0833 即为 A、3.1 动态查找与静态查找适于对动态查找表进行高效率查找的组织结构是分块有序表(错) 分块查找是静态查找 动态查找有二叉排序树查找，最优二叉树查找，键树查找，哈希表查找 静态查找表只进行以下2个操作： 1.查找某个“特定”数据元素是否在查找表中 2.查找某个“特定”数据元素的各种属性 有序表、分块有序表、线性链表都是静态查找表性能分析：平均查找长度：（当查找关键字等概率时）ASL = 1/(n+1) 动态查找表:表结构是在查找过程中动态生成的，通俗解释，对于给定key,若表中存在某关键字与key相等则查找成功返回，若未找到则插入关键字等于key的记录。 二叉排序树、平衡二叉树、B树、B+树都是动态查找。（对查找表进行插入和删除操作—即为动态的） 平均查找长度就平均查找长度而言,分块查找最小,折半查找次之,顺序查找最大(错) 分快查找，是将顺序表分为若干块，块内元素顺序任意，块间有序，即前一块中的最大值小于后一块中的最小值。并且有一张索引表，每一项存放每一块的最大值和指向该块第一个元素的指针。索引表有序，块内无序。所以，块间查找用二分查找，块内用顺序查找，效率介于顺序和二分之间。 分块查找：1.将顺序表分为若干块，除最后一块，前面每块元素相等，块间有序，块内无序2.索引表内元素有序，用二分折半查找，每块内元素无序，用顺序查找3.所以分块查找介于折半查找和顺序查找之间 判断是否有环判断是否有环方法：1.拓扑排序2.深度优先遍历3.广度优先遍历]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>堆</tag>
        <tag>栈</tag>
        <tag>图</tag>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer（二）:栈，队，数组]]></title>
    <url>%2Farchives%2Fc071cc58.html</url>
    <content type="text"><![CDATA[用两个栈来实现一个队列用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型 &lt;分析&gt;： 入队：将元素进栈A 出队：判断栈B是否为空，如果为空，则将栈A中所有元素pop，并push进栈B，栈B出栈； 如果不为空，栈B直接出栈。 用两个队列实现一个栈的功能?要求给出算法和思路! &lt;分析&gt;： 入栈：将元素进队列A 出栈：判断队列A中元素的个数是否为1，如果等于1，则出队列，否则将队列A中的元素 以此出队列并放入队列B，直到队列A中的元素留下一个，然后队列A出队列，再把 队列B中的元素出队列以此放入队列A中。1234567891011121314151617181920212223class Solution&#123;public: void push(int node)&#123; stack1.push(node); &#125; int pop()&#123; int a; if(stack2.empty())&#123; while(!stack1.empty())&#123; a=stack1.top(); stack2.push(a); stack1.pop(); &#125; &#125; a=stack2.top(); stack2.pop(); return a; &#125;private: stack&lt;int&gt; stack1; stack&lt;int&gt; stack2; &#125;; 算法思路： 栈A用来作入队列栈B用来出队列，当栈B为空时，栈A全部出栈到栈B,栈B再出栈（即出队列）12345678910111213# -*- coding:utf-8 -*-class Solution: def __init__(self): self.stack1=[] self.stack2=[] def push(self,node): self.stack1.append(node) def pop(self,node): if self.stack2==[]: while self.stack1: self.stack2.append(self.stack1,pop()) return stack2.pop() return stack2.pop() 旋转数组的最小数字把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。12345678910111213141516171819# -*- coding:utf-8 -*-class Solution: def minNumberInRotateArray(self, rotateArray): # write code here if len(rotateArray) == 0: return 0 ret = rotateArray[0] if len(rotateArray) == 1: return ret for i in range(1,len(rotateArray)): now = rotateArray[i] if now &lt; ret: ret = now break return ret 剑指Offer中有这道题目的分析。这是一道二分查找的变形的题目。 旋转之后的数组实际上可以划分成两个有序的子数组：前面子数组的大小都大于后面子数组中的元素 注意到实际上最小的元素就是两个子数组的分界线。本题目给出的数组一定程度上是排序的，因此我们试着用二分查找法寻找这个最小的元素。 思路： （1）我们用两个指针left,right分别指向数组的第一个元素和最后一个元素。按照题目的旋转的规则，第一个元素应该是大于最后一个元素的（没有重复的元素）。 但是如果不是旋转，第一个元素肯定小于最后一个元素。 （2）找到数组的中间元素。 中间元素大于第一个元素，则中间元素位于前面的递增子数组，此时最小元素位于中间元素的后面。我们可以让第一个指针left指向中间元素。 移动之后，第一个指针仍然位于前面的递增数组中。 中间元素小于第一个元素，则中间元素位于后面的递增子数组，此时最小元素位于中间元素的前面。我们可以让第二个指针right指向中间元素。 移动之后，第二个指针仍然位于后面的递增数组中。 这样可以缩小寻找的范围。 （3）按照以上思路，第一个指针left总是指向前面递增数组的元素，第二个指针right总是指向后面递增的数组元素。 最终第一个指针将指向前面数组的最后一个元素，第二个指针指向后面数组中的第一个元素。 也就是说他们将指向两个相邻的元素，而第二个指针指向的刚好是最小的元素，这就是循环的结束条件。 到目前为止以上思路很耗的解决了没有重复数字的情况，这一道题目添加上了这一要求，有了重复数字。 因此这一道题目比上一道题目多了些特殊情况： 我们看一组例子：｛1，0，1，1，1｝ 和 ｛1，1， 1，0，1｝ 都可以看成是递增排序数组｛0，1，1，1，1｝的旋转。 这种情况下我们无法继续用上一道题目的解法，去解决这道题目。因为在这两个数组中，第一个数字，最后一个数字，中间数字都是1。 第一种情况下，中间数字位于后面的子数组，第二种情况，中间数字位于前面的子数组。 因此当两个指针指向的数字和中间数字相同的时候，我们无法确定中间数字1是属于前面的子数组（绿色表示）还是属于后面的子数组（紫色表示）。 也就无法移动指针来缩小查找的范围。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;string&gt;#include &lt;stack&gt;#include &lt;algorithm&gt;using namespace std; class Solution &#123;public: int minNumberInRotateArray(vector&lt;int&gt; rotateArray) &#123; int size = rotateArray.size(); if(size == 0)&#123; return 0; &#125;//if int left = 0,right = size - 1; int mid = 0; // rotateArray[left] &gt;= rotateArray[right] 确保旋转 while(rotateArray[left] &gt;= rotateArray[right])&#123; // 分界点 if(right - left == 1)&#123; mid = right; break; &#125;//if mid = left + (right - left) / 2; // rotateArray[left] rotateArray[right] rotateArray[mid]三者相等 // 无法确定中间元素是属于前面还是后面的递增子数组 // 只能顺序查找 if(rotateArray[left] == rotateArray[right] &amp;&amp; rotateArray[left] == rotateArray[mid])&#123; return MinOrder(rotateArray,left,right); &#125;//if // 中间元素位于前面的递增子数组 // 此时最小元素位于中间元素的后面 if(rotateArray[mid] &gt;= rotateArray[left])&#123; left = mid; &#125;//if // 中间元素位于后面的递增子数组 // 此时最小元素位于中间元素的前面 else&#123; right = mid; &#125;//else &#125;//while return rotateArray[mid]; &#125;private: // 顺序寻找最小值 int MinOrder(vector&lt;int&gt; &amp;num,int left,int right)&#123; int result = num[left]; for(int i = left + 1;i &lt; right;++i)&#123; if(num[i] &lt; result)&#123; result = num[i]; &#125;//if &#125;//for return result; &#125;&#125;; int main()&#123; Solution s; //vector&lt;int&gt; num = &#123;0,1,2,3,4,5&#125;; //vector&lt;int&gt; num = &#123;4,5,6,7,1,2,3&#125;; vector&lt;int&gt; num = &#123;2,2,2,2,1,2&#125;; int result = s.minNumberInRotateArray(num); // 输出 cout&lt;&lt;result&lt;&lt;endl; return 0;&#125;]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>查找</tag>
        <tag>替换空格</tag>
        <tag>打印链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer（一）]]></title>
    <url>%2Farchives%2F8ad708de.html</url>
    <content type="text"><![CDATA[二维数组中的查找在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 算法思想： 由于数组从左到右，从上到下都是递增，可以从右上角或者左下角开始查找，这里从右上角开始查找 定义数组array，行i，列j，目标target。 若array[i][j]==target,那恭喜你找到目标； 若array[i][j]&gt;target,那向左去找目标–j； 否则，array[i][j]小于target,那向下寻找目标++i； 1234567891011121314151617181920class Solution&#123;public: bool Find(int target,vector&lt;vector&lt;int&gt; &gt; array)&#123; if(array.size()!=0) &#123; int i=0; int j=array[0].size()-1; while(i&lt; array.size()&amp;&amp; j&gt;= 0) &#123; if(array[i][j]== target) return true; else if(array[i][j]&gt;target) --j; else ++i; &#125; &#125; return false; &#125;&#125;; 替换空格请实现一个函数，将一个字符串中的空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy 算法思想： 定义字符串下标i（i=0开始），定义空格bankNum,遍历字符串并计算空格个数； 总字符串=原始字符串下标+1+2*bankNum； 末尾字符串下标=totalNum-1，也就是我们说的放置位置； 总体来说：从后向前遍历字符串，如果遇到空格，就加%20；没有空格，就将原字符串的最后一个字符赋给新字符串 123456789101112131415161718192021222324252627class Solution&#123;public: void replaceSpaca(char *str,int length) &#123; int bankNum=0; int i=0; for(;str[i]!='\0';++i) if(str[i]==' ') ++bankNum; &#125; int totalNum=i+1+2*bankNum; int pos=totalNum-1; for(;i&gt;=0;--i) &#123; if(str[i]==' ') &#123; str[pos--]='0'; str[pos--]='2'; str[pos--]='%'; &#125; else str[pos--]=str[i]; &#125;; &#125; 打印链表输入一个链表，从尾到头打印链表每个节点的值。 12345678class Solution: def printListFromTailToHead(self,listNode): l=[] head = listNode while head: l.insert(0,head.val) head=head.next return 1 二分查找1234567891011121314151617181920def search2(a,m): low=0 high=len(a)-1 while(low&lt;=high): mid=(low+high)/2 midval=a[mid] if midval&lt;m: low=mid+1 else if midval&gt;m: high=mid-1 else return mid return -1 if __name__=="__main__" a=[1,2,3,4,5,6,7,8,9] m=5 result=search2(a,m) print result 下面这个程序就会陷入无限循环while中123456789101112131415161718192021#include&lt;studio.h&gt;int bsearch(int a[],int n,int v)&#123; int left, middle, right; left=0, right=n-1; while(left&lt;=right) &#123; middle=left+(right-left)/2; if(a[middle]&gt;v) &#123; right=middle &#125; else if(a[middle]&lt;v) &#123; left=middle &#125; else return middle; &#125; return -1;&#125; 以下是正确写法123456789101112131415161718public static int binarySearch(int[] a,int n,int x)&#123; if(n&gt;0 &amp;&amp; x&gt;=a[0]) &#123; int left=0,right=n-1; while(left&lt;right) &#123; int middle=left+(right-left)/2; if(x&lt;a[middle]) right=middle-1; else left=middle; &#125; if(x==a[left]) return left; &#125; return -1;&#125; 重建二叉树输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。1234567891011121314151617# -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: # 返回构造的TreeNode根节点 def reConstructBinaryTree(self, pre, tin): # write code here if len(pre) == 0: return None root = TreeNode(pre[0]) pos = tin.index(pre[0]) root.left = self.reConstructBinaryTree( pre[1:1+pos], tin[:pos]) root.right = self.reConstructBinaryTree( pre[pos+1:], tin[pos+1:]) return root]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>查找</tag>
        <tag>替换空格</tag>
        <tag>打印链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构（一）]]></title>
    <url>%2Farchives%2F167cd958.html</url>
    <content type="text"><![CDATA[字符串设字符串S=’ABCDEFG’,T=’PQRST’,则运算CONCAT(SUBSTR(S,2，LENGTH(T),SUBSTR(S,LENGTH(T),2) ))后的结果为（）A.’BCQR’B.’BCDEF’C.’BCDEFG’D.’BCDEFEF’ oracle中取得字符串中指定起始位置和长度的字符串substr( string, start_position, [ length ] )，计数从1开始，字符串长度不带“\0” concat() 方法用于连接两个或多个数组。 substr是C++语言函数，主要功能是复制子字符串，要求从指定位置开始，并具有指定的长度。 SUBSTR(S,2，LENGTH(T) ) 如果没有指定长度_Count或_Count+_Off超出了源字符串的长度，则子字符串将延续到源字符串的结尾。oracle中字符下标从1开始，此处为从字符串S的第2个开始，截取长度为LENGTH(T)即5的字符串结果为：BCDEFSUBSTR(S,LENGTH(T),2)，从字符串S的第LENGTH(T)即第5个开始，截取长度为2的字符串结果为：EFCONCAT结果为：BCDEFEF 下面关于字符串的描述正确的是：【多选】（ ）A.通过String s1=new String(“abc”)和String s2=”abc”，则s1==s2为true。B.”abc”+”def”则会创建三个字符串对象，第三个是”abcdef”。也就是说，在Java中对字符串的一切操作，都会产生一个新的字符串对象。C.StringBuffer是线程安全的，它比String快。D.StringBuilder是线程安全的，它比String快 三者执行速度：StringBuilder &gt; StringBuffer &gt; String ； StringBuilder：线程非安全的； StringBuffer：线程安全的； 用String操作字符串时，实际上是在不断地创建新对象，而原来的对象会作为垃圾被回收； 对于A： s1利用new 操作后，为该对象在堆（Heap）区分配了一块内存； s2是字符串常量，存放在内存的”文字常量区“ ；虽然两个对象的值相同，但由于两者位于不同的地址，不是相同的对象，因此 s1==s2 为false。 A错 多型数据类型 多型数据类型是指包含的数据元素的类型并不确定。 比如栈可以是整数栈、字符栈、对象栈等等。 但是字符串，它的元素必然是字符。 设模式串的长度为m,目标串的长度为n,当n≈m且处理只匹配一次的模式时,朴素的匹配(即子串定位函数)算法所花的时间代价可能会更为节省()A.对B.错 朴素的匹配只匹配一次，不用计算next数组，所以速度更快 字符串的朴素算法(就是暴力搜索） 子串 n 个字符构成的字符串，假设每个字符都不一样，问有多少个子串？A. n+1B.n(n+1)/2 + 1C.2^n-1D.n! 这么想就很简单：长度为 1 的字符串 n 个长度为 2 的 n-1 个长度为 3 的 n-2 个…长度为 n 的 1 个然后 n+(n-1)+(n-2)+…+1 =n(n+1)/2 下面程序段的输出结果是D 123char *p1 = ”123”, *p2 = ”ABC”, str[50] = “xyz”;strcpy(str + 2, strcat(p1, p2));printf(“%s\n”, str); A.xyz123ABCB.z123ABCC.xy123ABCD.出错 原代码有错： p1和p2都指向常量字符串，在常量区，所以不能对其进行操作； 改为数组即可，但是用字符串初始化数组时要记得将数组长度加1，因为字符串默认的末尾有一个‘\0’； 第二点要注意的是，strcat函数的p1要有足够的空间来容纳p1和p2连接后的串长。 修改为以下代码将可以：12345char p1[7] = "123";char p2[] = "ABC";char str[50] = "xyz";strcpy(str + 2, strcat(p1, p2));printf("%s\n", str); char *p1=”123” 声明了个字符串指针p1，指向字符串“ 123 ”，此时的“ 123 ”存放在常量区，并没有在拷贝到栈中，所以不能修改，如修改p1[0] = ‘2’就是错误的。建议改为char p1[10] = “123”，就可以修改p1的值。 空串和由空格组成的串 空串：a=“没有东西”； 空格串：b=“空格空格” 所以空串和空格串是不一样的 想像使用split()方法时，这两个参数得到的结果肯定不一样 #有关赋值不能所字符串“Good!”存放到数组 s 中的代码是（D）A.char s[8] = {‘G’,’o’,’o’,’d’,’!’, ‘\0’};B.char s[8];strcpy(s, “Good!”);C.char s[8];s = “Good!”;D.char s[8] = “Good!”; char数组只有在初始化的时候才能整体赋值 char s[8],表示s是一个不可修改的左值，s实际上是char *const s 类型的值 KMP KMP算法的特点是在模式匹配时指示主串的指针不会变小 KMP算法最大的特点就是指示主串的指针不需要回溯，因此指针不可能变小 NEXT数组串′ababaaababaa′的next数组为(011234223456) 第一种方法： next数组的求解方法是：第一位的next值为0，第二位的next值为1，后面求解每一位的next值时，根据前一位进行比较。 首先将前一位与其next值对应的内容进行比较，如果相等，则该位的next值就是前一位的next值加上1； 如果不等，向前继续寻找next值对应的内容来与前一位进行比较，直到找到某个位上内容的next值对应的内容与前一位相等为止，则这个位对应的值加上1即为需求的next值； 如果找到第一位都没有找到与前一位相等的内容，那么需求的位上的next值即为1。 第二种解释：next数组下标从1开始计算next[1] 肯定是 0next[2] 肯定是 1next[n] 的情况，将前面n-1个字符，计算从首尾开始组成最大的相同子串的长度，如果找到，那么next值是该长度加1，否则next值是1。 举例next[6]的计算，字符串第六位是 a ，( ababa a ababaa)将前面的5个字符，从头尾开始取4个组成子串比较，如果不相等，则从首尾取3个字符组成子串继续比较，并以此类推， 如果一直比较到最后一个字符都不相等，那么该next值为1。4个字符的情况：abab : baba3个字符的情况：aba : aba 此时相等，那么next[6] = 3+1 = 4 第三种（在看不懂我就没办法了） 123 i 0 1 2 3 4 5 6 7 8 9 10 11 s a b a b a a a b a b a a next[i] -1 0 0 1 2 3 1 1 2 3 4 5 先计算前缀next[i]的值： （字符串匹配是 从头开始的 和 从尾开始的字符串进行匹配是否重复 ） next[i]的值主要是看s[i]之前的字符串中重复的子串长度。next[0] = -1，定值。 next[1]是看s[1]之前的字符串“a”中重复的子串长度为0，故next[1] = 0。 next[2]是看s[2]之前的字符串“ab”中重复的子串长度为0，故next[2] = 0。 next[3]是看s[3]之前的字符串”aba”中重复的子串长度，s[0]与s[2]重复，长度为1，故next[3] = 1。 next[4]是看s[4]之前的字符串”abab”中重复的子串长度，s[01]与s[23]重复，长度为2，故next[4] = 2。 next[5]是看s[5]之前的字符串”ababa”中重复的子串长度，s[012]与s[234]重复，长度为3，故next[5] = 3。 next[6]是看s[6]之前的字符串”ababaa”中重复的子串长度，s[0]与s[5]重复(因为多了一个a，无法找到长度为3的重复字符串，这只能是s[0]和s[5]重复)，长度为1，故next[6] = 1。 同样的，求next[7]和next[8]、next[9]、 next[10]、 next[11] 分别为1和2、3、4、5。 指针和数组12345678910111213141516#include&lt;stdio.h&gt;char *myString()&#123; char buffer[6] = &#123;0&#125;; char *s = &quot;Hello World!&quot;; for (int i = 0; i &lt; sizeof(buffer) - 1; i++) &#123; buffer[i] = *(s + i); &#125; return buffer;&#125;int main(int argc, char **argv)&#123; printf(&quot;%s\n&quot;, myString()); return 0;&#125; 输出结果？ 函数char *myString()中没有使用new或者malloc分配内存，所有buffer数组的内存区域在栈区 随着char *myString()的结束，栈区内存释放，字符数组也就不存在了，所以会产生野指针，输出结果未知]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>堆</tag>
        <tag>栈</tag>
        <tag>图</tag>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（四）：AdaBoost算法 Boosting和Bagging等]]></title>
    <url>%2Farchives%2F919c217.html</url>
    <content type="text"><![CDATA[SVM以下关于SVM说法正确的是（）A.L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力B.Hinge 损失函数，作用是最小化经验分类错误C.分类间隔为1/||w||，||w||代表向量的模D.当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习 (1). 考虑加入正则化项的原因：想象一个完美的数据集，y&gt;1是正类，y&lt;-1是负类，决策面y=0，加入一个y=-30的正类噪声样本，那么决策面将会变“歪”很多，分类间隔变小，泛化能力减小。A正确。 (2). 加入正则项之后，对噪声样本的容错能力增强，前面提到的例子里面，决策面就会没那么“歪”了，使得分类间隔变大，提高了泛化能力。 B正确。 (3). C错误。间隔应该是2/||w||才对，后半句应该没错，向量的模通常指的就是其二范数。 (4). D正确。考虑软间隔的时候，C对优化问题的影响就在于把a的范围从[0，+inf]限制到了[0,C]。C越小，那么a就会越小，目标函数拉格朗日函数导数为0可以求出w=求和a_iy_ix_i，a变小使得w变小，因此间隔2/||w||变大 过拟合在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题（）A.增加训练集量B.减少神经网络隐藏层节点数C.删除稀疏的特征SD.SVM算法中使用高斯核/RBF核代替线性核 (1). 一般认为，增加隐层数可以降低网络误差（也有文献认为不一定能有效降低），提高精度，但也使网络复杂化，从而增加了网络的训练时间和出现“过拟合”的倾向， (2). 引起过拟合的应该是太多的参数引起的。神经网络减少隐藏层节点，就是在减少参数啊，只会将训练误差变高，怎么会过拟合呢。 B错误。 (3). 径向基(RBF)核函数/高斯核函数的说明 a. 这个核函数可以将原始空间映射到无穷维空间。 b. 对于参数 ，如果选的很大，高次特征上的权重实际上衰减得非常快，实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。 c. 不过，总的来说，通过调整参数 ，高斯核实际上具有相当高的灵活性，也是 使用最广泛的核函数 之一。 (4). D正确。SVM高斯核函数比线性核函数模型更复杂，容易过拟合。 数据清理数据清理中，处理缺失值的方法有两种： 删除法： 1）删除观察样本 2）删除变量：当某个变量缺失值较多且对研究目标影响不大时，可以将整个变量整体删除 3）使用完整原始数据分析：当数据存在较多缺失而其原始数据完整时，可以使用原始数据替代现有数据进行分析 4）改变权重：当删除缺失数据会改变数据结构时，通过对完整数据按照不同的权重进行加权，可以降低删除缺失数据带来的偏差 查补法：均值插补、回归插补、抽样填补等成对删除与改变权重为一类估算与查补法为一类 由于调查、编码和录入误差，数据中可能存在一些无效值和缺失值，需要给予适当的处理。常用的处理方法有：估算，整例删除，变量删除和成对删除。 估算(estimation)。最简单的办法就是用某个变量的样本均值、中位数或众数代替无效值和缺失值。这种办法简单，但没有充分考虑数据中已有的信息，误差可能较大。另一种办法就是根据调查对象对其他问题的答案，通过变量之间的相关分析或逻辑推论进行估计。例如，某一产品的拥有情况可能与家庭收入有关，可以根据调查对象的家庭收入推算拥有这一产品的可能性。 整例删除(casewise deletion)是剔除含有缺失值的样本。由于很多问卷都可能存在缺失值，这种做法的结果可能导致有效样本量大大减少，无法充分利用已经收集到的数据。因此，只适合关键变量缺失，或者含有无效值或缺失值的样本比重很小的情况。 变量删除(variable deletion)。如果某一变量的无效值和缺失值很多，而且该变量对于所研究的问题不是特别重要，则可以考虑将该变量删除。这种做法减少了供分析用的变量数目，但没有改变样本量。 成对删除(pairwise deletion)是用一个特殊码(通常是9、99、999等)代表无效值和缺失值，同时保留数据集中的全部变量和样本。但是，在具体计算时只采用有完整答案的样本，因而不同的分析因涉及的变量不同，其有效样本量也会有所不同。这是一种保守的处理方法，最大限度地保留了数据集中的可用信息。 采用不同的处理方法可能对分析结果产生影响，尤其是当缺失值的出现并非随机且变量之间明显相关时。因此，在调查中应当尽量避免出现无效值和缺失值，保证数据的完整性。 分支定界法 分支定界法（branch and bound）是一种求解 整数规划 问题的最常用算法。 这种方法不但可以求解纯整数规划，还可以求解混合整数规划问题。 分支定界法是计算机最擅长 的广义搜索穷举算法。 分支定界法是一种搜索与迭代的方法，选择不同的分支变量和子问题进行分支。 对于两个变量的整数规划问题，使用网格的方法有时更为简单。 分支定界法类似决策树的决策特征，要选择那些具有强可分辨性的少量特征。 线性回归关于线性回归的描述,以下正确的有:2,3,5 基本假设包括随机干扰项是均值为0,方差为1的标准正态分布 基本假设包括随机干扰项是均值为0的同方差正态分布 在违背基本假设时,普通最小二乘法估计量不再是最佳线性无偏估计量 在违背基本假设时,模型不再可以估计 可以用DW检验残差是否存在序列相关性 多重共线性会使得参数估计值方差减小 一元线性回归的基本假设有: 随机误差项是一个期望值或平均值为0的随机变量； 对于解释变量的所有观测值，随机误差项有相同的方差； 随机误差项彼此不相关； 解释变量是确定性变量，不是随机变量，与随机误差项彼此之间相互独立； 解释变量之间不存在精确的（完全的）线性关系，即解释变量的样本观测值矩阵是满秩矩阵； 随机误差项服从正态分布 注意： 违背基本假设的计量经济学模型还是可以估计的，只是不能使用普通最小二乘法进行估计。 当存在异方差时，普通最小二乘法估计存在以下问题： 参数估计值虽然是无偏的，但不是最小方差线性无偏估计。 杜宾-瓦特森（DW）检验，计量经济，统计分析中常用的一种检验序列一阶 自相关 最常用的方法。 所谓多重共线性（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。影响 （1）完全共线性下参数估计量不存在 （2）近似共线性下OLS估计量非有效 多重共线性使参数估计值的方差增大，1/(1-r2)为方差膨胀因子(Variance Inflation Factor, VIF) （3）参数估计量经济含义不合理 （4）变量的显著性检验失去意义，可能将重要的解释变量排除在模型之外 （5）模型的预测功能失效。变大的方差容易使区间预测的“区间”变大，使预测失去意义。 SVM AdaBoost算法 Boosting和Bagging以下说法中正确的是(BD)A.SVM对噪声(如来自其他分布的噪声样本)鲁棒B.在AdaBoost算法中,所有被分错的样本的权重更新比例相同C.Boosting和Bagging都是组合多个分类器投票的方法,二者都是根据单个分类器的正确率决定其权重D.给定n个数据点,如果其中一半用于训练,一般用于测试,则训练误差和测试误差之间的差别会随着n的增加而减少 Adaboost目的是从训练数据中学习一系列弱分类器，然后将其按一定权重累加起来得到强分类器。刚开始每个样本对应的权重是相等的，在此样本分布下训练一个基本分类器c1.对于c1错分的样本增加其权重，对正确分类的样本降低其权重。这样使得错分的样本突出出来，并得到一个新的样本分布。同时根据分类情况赋予c1一个权重，表示其重要程度，分类正确率越高权重越大。然后在新的样本分布下对分类器进行训练，得到c2及其权重。依此类推，得到M个基本分类器及其权重。将这些弱分类器按照权重累加起来就是所期望的强分类器。（B对） Bagging是对训练样本多次抽样训练多个分类器，然后对测试集进行投票所得到的优胜结果就是最终的分类结果。在投票时每个分类器的权重是相等的。（所以C错） SVM对噪声（如来自其他分布的噪声样本）鲁棒。SVM本身对噪声具有一定的鲁棒性，但实验证明，是当噪声率低于一定水平的噪声对SVM没有太大影响，但随着噪声率的不断增加，分类器的识别率会降低。 在AdaBoost算法中所有被分错的样本的权重更新比例相同。AdaBoost算法中不同的训练集是通过调整每个样本对应的权重来实现的。开始时，每个样本对应的权重是相同的，即其中n为样本个数，在此样本分布下训练出一弱分类器。对于分类错误的样本，加大其对应的权重；而对于分类正确的样本，降低其权重，这样分错的样本就被凸显出来，从而得到一个新的样本分布。在新的样本分布下，再次对样本进行训练，得到弱分类器。以此类推，将所有的弱分类器重叠加起来，得到强分类器。 Boost和Bagging都是组合多个分类器投票的方法，二者均是根据单个分类器正确率决定其权重。 Bagging与Boosting的区别：取样方式不同。 Bagging采用均匀取样，而Boosting根据错误率取样。 Bagging的各个预测函数没有权重，而Boosting是由权重的，Bagging的各个预测函数可以并行生成，而Boosing的哥哥预测函数只能顺序生成。 判别式模型与生成式模型生成式模型(Generative Model)与判别式模型(Discrimitive Model)是分类器常遇到的概念，它们的区别在于：（对于输入x，类别标签y） 生成式模型估计它们的联合概率分布P(x,y) 判别式模型估计决策函数F(X)或条件概率分布P(y|x) 生成式式模型可以根据贝叶斯公式得到判别式模型，但反过来不行 生成式模型 判别式分析 朴素贝叶斯Native Bayes 混合高斯型Gaussians K近邻KNN 隐马尔科夫模型HMM 贝叶斯网络 sigmoid belief networks 马尔科夫随机场Markov random fields 深度信念网络DBN 隐含狄利克雷分布简称LDA(Latent Dirichlet allocation) 多专家模型（the mixture of experts model） 判别式模型 线性回归linear regression 逻辑回归logic regression 神经网络NN 支持向量机SVM 高斯过程Gaussian process 条件随机场CRF CART(Classification and regression tree) Boosting 线性分类器线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。 感知器准则函数：代价函数J=-(W*X+w0).分类的准则是最小化代价函数。感知器是神经网络（NN）的基础，网上有很多介绍。 SVM：支持向量机也是很经典的算法，优化目标是最大化间隔（margin），又称最大间隔分类器，是一种典型的线性分类器。（使用核函数可解决非线性问题） Fisher准则：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。 贝叶斯分类器：一种基于统计方法的分类器，要求先了解样本的分布特点（高斯、指数等），所以使用起来限制很多。在满足一些特定条件下，其优化目标与线性分类器有相同结构（同方差高斯分布等），其余条件下不是线性分类。 分类问题在分类问题中,我们经常会遇到正负样本数据量不等的情况,比如正样本为10w条数据,负样本只有1w条数据,以下最合适的处理方法是()A.将负样本重复10次,生成10w样本量,打乱顺序参与分类B.直接进行分类,可以最大限度利用数据C.从10w正样本中随机抽取1w参与分类D.将负样本每个权重设置为10,正样本权重为1,参与训练过程 解决这类问题主要分： 重采样。A可视作重采样的变形。改变数据分布消除不平衡，可能导致过拟合。 欠采样。C的方案 提高少数类的分类性能，可能丢失多数类的重要信息。 权值调整。 如果1：10算是均匀的话，可以将多数类分割成为1000份。然后将每一份跟少数类的样本组合进行训练得到分类器。而后将这1000个分类器用assemble的方法组合位一个分类器。A选项可以看作此方式，因而相对比较合理。 另：如果目标是 预测的分布 跟训练的分布一致，那就加大对分布不一致的惩罚系数。 D方案也是其中一种方式。 类域界面方程法类域界面方程法中，不能求线性不可分情况下分类问题近似或精确解的方法是？A.伪逆法B.感知器算法C.基于二次准则的H-K算法D.势函数法 伪逆法：径向基（RBF）神经网络的训练算法，径向基解决的就是线性不可分的情况。 感知器算法：线性分类模型。线性不可分时，感知器算法不收敛。 H-K算法：在最小均方误差准则下求得权矢量，二次准则解决非线性问题。 基于二次准则函数的H-K算法较之于感知器算法的优点是:可以判别问题是否线性可分,其解的适应性更好. 势函数法：势函数非线性。 主分量（主成分）分析PCA已知一组数据的协方差矩阵P,下面关于主分量说法错误的是(C)A.主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小B.在经主分量分解后,协方差矩阵成为对角矩阵C.主分量分析就是K-L变换D.主分量是通过求协方差矩阵的特征值得到 K-L变换与PCA变换是不同的概念 PCA的变换矩阵是协方差矩阵 K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等） 当K-L变换矩阵为协方差矩阵时，等同于PCA。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（三）：SVM核函数等]]></title>
    <url>%2Farchives%2F8823626c.html</url>
    <content type="text"><![CDATA[SVMSVM核函数包括： 线性核函数、 多项式核函数、 径向基核函数、 高斯核函数、 幂指数核函数、 拉普拉斯核函数、 ANOVA核函数、 二次有理核函数、 多元二次核函数、 逆多元二次核函数 Sigmoid核函数 支持向量机是建立在统计学习理论基础之上的新一代机器学习算法，支持向量机的优势主要体现在解决线性不可分问题，它通过引入核函数，巧妙地解决了在高维空间中的内积运算，从而很好地解决了非线性分类问题。 构造出一个具有良好性能的SVM，核函数的选择是关键．核函数的选择包括两部分工作：一是核函数类型的选择，二是确定核函数类型后相关参数的选择． 因此如何根据具体的数据选择恰当的核函数是SVM应用领域遇到的一个重大难题，也成为科研工作者所关注的焦点，即便如此，却依然没有得到具体的理论或方法来指导核函数的选取． 1、经常使用的核函数核函数的定义并不困难，根据泛函的有关理论，只要一种函数 K ( x i , x j ) 满足Mercer条件，它就对应某一变换空间的内积．对于判断哪些函数是核函数到目前为止也取得了重要的突破，得到Mercer定理和以下常用的核函数类型： (1)线性核函数 K ( x , x i ) = x ⋅ x i (2)多项式核 K ( x , x i ) = ( ( x ⋅ x i ) + 1 ) d (3)径向基核（RBF） K ( x , x i ) = exp ( − ∥ x − x i ∥ 2 σ 2 ) Gauss径向基函数则是局部性强的核函数，其外推能力随着参数 σ 的增大而减弱。多项式形式的核函数具有良好的全局性质。局部性较差。 (4)傅里叶核 K ( x , x i ) = 1 − q 2 2 ( 1 − 2 q cos ( x − x i ) + q 2 ) (5)样条核 K ( x , x i ) = B 2 n + 1 ( x − x i ) (6)Sigmoid核函数 K ( x , x i ) = tanh ( κ ( x , x i ) − δ ) 采用Sigmoid函数作为核函数时，支持向量机实现的就是一种多层感知器神经网络，应用SVM方法，隐含层节点数目(它确定神经网络的结构)、隐含层节点对输入节点的权值都是在设计(训练)的过程中自动确定的。而且支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最小值，也保证了它对于未知样本的良好泛化能力而不会出现过学习现象。 2、核函数的选择在选取核函数解决实际问题时，通常采用的方法有： 一是利用专家的先验知识预先选定核函数； 二是采用Cross-Validation方法，即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数．如针对傅立叶核、RBF核，结合信号处理问题中的函数回归问题，通过仿真实验，对比分析了在相同数据条件下，采用傅立叶核的SVM要比采用RBF核的SVM误差小很多; 三是采用由Smits等人提出的混合核函数方法，该方法较之前两者是目前选取核函数的主流方法，也是关于如何构造核函数的又一开创性的工作．将不同的核函数结合起来后会有更好的特性，这是混合核函数方法的基本思想． HMMQ:如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计(D)A.EM算法B.维特比算法C.前向后向算法D.极大似然估计 EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法 维特比算法： 用动态规划解决HMM的预测问题，不是参数估计 维特比算法解决的是给定 一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。如通过海藻变化（输出序列）来观测天气（状态序列），是预测问题，通信中的解码问题 Baum-Welch算法解决的是一个模型训练问题，即参数估计，是一种无监督的训练方法，主要通过EM迭代实现； 前向后向：用来算概率 极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数 在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。 如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据 特征提取算法特征提取算法分为特征选择和特征抽取两大类 特征选择常采用特征选择方法。常见的六种特征选择方法： DF(Document Frequency) 文档频率DF:统计特征词出现的文档数量，用来衡量某个特征词的重要性 MI(Mutual Information) 互信息法互信息法用于衡量特征词与文档类别直接的信息量。如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向”低频”的特征词。相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。 (Information Gain) 信息增益法通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。 CHI(Chi-square) 卡方检验法利用了统计学中的”假设检验”的基本思想：首先假设特征词与类别直接是不相关的如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。 WLLR(Weighted Log Likelihood Ration)加权对数似然 WFO（Weighted Frequency and Odds）加权频率和可能性 特征抽取（降维） PCA logit 回归和 SVM关于 logit 回归和 SVM 不正确的是（A） A.Logit回归目标函数是最小化后验概率B.Logit回归可以用于预测事件发生概率的大小C.SVM目标是结构风险最小化D.SVM可以有效避免模型过拟合 A. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。A错误 B. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确 C. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化，严格来说也是错误的。 D. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。 L0范数与L1范数，L2范数L0范数L0范数是指向量中非0的元素的个数。 如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。这太直观了，太露骨了吧，换句话说，让参数W是稀疏的。OK，看到了“稀疏”二字，大家都应该从当下风风火火的“压缩感知”和“稀疏编码”中醒悟过来，原来用的漫山遍野的“稀疏”就是通过这玩意来实现的。 但你又开始怀疑了，是这样吗？看到的papers世界中，稀疏不是都通过L1范数来实现吗？脑海里是不是到处都是||W||1影子呀！几乎是抬头不见低头见。没错，这就是这节的题目把L0和L1放在一起的原因，因为他们有着某种不寻常的关系。那我们再来看看L1范数是什么？它为什么可以实现稀疏？为什么大家都用L1范数去实现稀疏，而不是L0范数呢？ L1范数L1范数是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。 现在我们来分析下这个价值一个亿的问题：为什么L1范数会使权值稀疏？有人可能会这样给你回答“它是L0范数的最优凸近似”。 实际上，还存在一个更美的回答：任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。 这说是这么说，W的L1范数是绝对值，|w|在w=0处是不可微，但这还是不够直观。这里因为我们需要和L2范数进行对比分析。所以关于L1范数的直观理解，请待会看看第二节。 对了，上面还有一个问题：既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解 一是因为L0范数很难优化求解（NP难问题）二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。所以大家才把目光和万千宠爱转于L1范数。 一句话总结：L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。 为什么要稀疏？让我们的参数稀疏有什么好处呢？这里扯两点： 特征选择(Feature Selection)： 大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。 一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确yi的预测。 稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。 可解释性(Interpretability)： 另一个青睐于稀疏的理由是，模型更容易解释。 例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。 假设我们这个是个回归模型： y=w1x1+w2x2+…+w1000*x1000+b （当然了，为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。 通过学习，如果最后学习到的w*就只有很少的非零元素，例如只有5个非零的wi，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个wi都非0，医生面对这1000种因素，累觉不爱。 L2范数除了L1范数，还有一种更受宠幸的规则化范数是L2范数: ||W||2。它也不逊于L1范数，它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”。 这用的很多吧，因为它的强大功效是改善机器学习里面一个非常重要的问题：过拟合。 至于过拟合是什么，上面也解释了，就是模型训练时候的误差很小，但在测试的时候误差很大，也就是我们的模型复杂到可以拟合到我们的所有训练样本了，但在实际预测新的样本的时候，糟糕的一塌糊涂。 通俗的讲就是应试能力很强，实际应用能力很差。擅长背诵知识，却不懂得灵活利用知识。 L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别的哦。 而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。为什么越小的参数说明模型越简单？ 我也不懂，我的理解是：限制了参数很小，实际上就限制了多项式某些分量的影响很小（看上面线性回归的模型的那个拟合的图），这样就相当于减少参数个数。其实我也不太懂，希望大家可以指点下。 一句话总结下：通过L2范数，我们可以实现了对模型空间的限制，从而在一定程度上避免了过拟合。 L2范数的好处是什么呢？这里也扯上两点：1）学习理论的角度： 从学习理论的角度来说，L2范数可以防止过拟合，提升模型的泛化能力。 2）优化计算的角度： 从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。哎，等等，这condition number是啥？我先google一下哈。 这里我们也故作高雅的来聊聊优化问题。优化有两大难题，一是：局部最小值，二是：ill-condition病态问题。 前者俺就不说了，大家都懂吧，我们要找的是全局最小值，如果局部最小值太多，那我们的优化算法就很容易陷入局部最小而不能自拔，这很明显不是观众愿意看到的剧情。 那下面我们来聊聊ill-condition。ill-condition对应的是well-condition。那他们分别代表什么？假设我们有个方程组AX=b，我们需要求解X。如果A或者b稍微的改变，会使得X的解发生很大的改变，那么这个方程组系统就是ill-condition的，反之就是well-condition的。 一句话总结：conditionnumber是一个矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，如果一个矩阵的condition number在1附近，那么它就是well-conditioned的，如果远大于1，那么它就是ill-conditioned的，如果一个系统是ill-conditioned的，它的输出结果就不要太相信了。 从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。 总结吧：L2范数不但可以防止过拟合，还可以让我们的优化求解变得稳定和快速。 L1和L2的差别为什么一个让绝对值最小，一个让平方最小，会有那么大的差别呢？我看到的有两种几何上直观的解析： 下降速度： 我们知道，L1和L2都是规则化的方式，我们将权值参数以L1或者L2的方式放到代价函数里面去。然后模型就会尝试去最小化这些权值参数。而这个最小化就像一个下坡的 过程，L1和L2的差别就在于这个“坡”不同，如下图：L1就是按绝对值函数的“坡”下降的，而L2是按二次函数的“坡”下降。所以实际上在0附近，L1的下降速度比L2的下 降速度要快。所以会非常快得降到0。不过我觉得这里解释的不太中肯，当然了也不知道是不是自己理解的问题。 模型空间的限制： 实际上，对于L1和L2规则化的代价函数来说，也就是说，我们将模型空间限制在w的一个L1-ball 中。为了便于可视化，我们考虑两维的情况，在(w1, w2)平面上可以 画出目标函数的等高线，而约束条件则成为平面上半径为C的一个 norm ball 。等高线与 norm ball 首次相交的地方就是最优解 一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（二）：常识性知识]]></title>
    <url>%2Farchives%2Ffe892b9e.html</url>
    <content type="text"><![CDATA[计算输出特征图大小Q： 输入图片大小为200×200，依次经过一层卷积（kernel size 5×5，padding 1，stride 2），pooling（kernel size 3×3，padding 0，stride 1），又一层卷积（kernel size 3×3，padding 1，stride 1）之后，输出特征图大小为多少？ 公式：输出尺寸=(输入尺寸-filter尺寸+2*padding）/stride+1 A: 输出为97×97 1层卷积,输出为99×99：（200-5+2）/2+1=99.5 池化,输出为97×97： （99-3+0）/1+1=97 2层卷积,输出为97×97： （97-3+2）/1+1=97 计算尺寸不被整除只在GoogLeNet中遇到过。卷积向下取整，池化向上取整。 研究过网络的话看到stride为1的时候，当kernel为 3 padding为1或者kernel为5 padding为2 一看就是卷积前后尺寸不变。 计算GoogLeNet全过程的尺寸也一样。 SPSS（Statistical Product and Service Solutions） 统计产品与服务解决方案 SPSS为IBM公司推出的一系列用于统计学分析运算、数据挖掘、预测分析和决策支持任务的软件产品及相关服务的总称。 SPSS是世界上最早采用图形菜单驱动界面的统计软件，它最突出的特点就是操作界面极为友好，输出结果美观漂亮。。 SPSS的界面中，主窗口是: 数据编辑窗口。 在spss的基础分析模块中，作用是“以行列表的形式揭示数据之间的关系”的是交叉表。 spss中交叉分析主要用来检验两个变量之间是否存在关系，或者说是否独立，其零假设为两个变量之间没有关系。在实际工作中，经常用交叉表来分析比例是否相等。例如分析不同的性别对不同的报纸的选择有什么不同。 有关过拟合与逻辑回归Q: 在Logistic Regression 中,如果同时加入L1和L2范数,会产生什么效果？ A: 可以做特征选择,并在一定程度上防止过拟合. 做特征选择看可以使用L1，L2范数，具体如下： L1范数具有系数解的特性，但是要注意的是，L1没有选到的特征不代表不重要，原因是两个高相关性的特征可能只保留一个。如果需要确定哪个特征重要，再通过交叉验证。 在代价函数后面加上正则项，Ｌ１即是Ｌｏｓｓｏ回归，Ｌ２是岭回归. 但是它为什么能防止过拟合呢？ 奥卡姆剃刀原理：能很好的拟合数据且模型简单 模型参数在更新时，正则项可使参数的绝对值趋于０，使得部分参数为０，降低了模型的复杂度（模型的复杂度由参数决定），从而防止了过拟合。提高模型的泛化能力. L1范数是指向量中各个元素绝对值之和，用于特征选择 L2范数 是指向量各元素的平方和然后求平方根，用于 防止过拟合，提升模型的泛化能力 矩阵相乘Q: 现在需要计算三个稠密矩阵A,B,C的乘积ABC，假设三个矩阵的尺寸分别为mn,np,p*q,且m&lt;n&lt;p&lt;q，以下计算顺序效率最高的是：A. A(BC) B.(AB)C C.(AC)B D.所有效率都相同A: B ab,bc两矩阵相乘效率为acbABC=(AB)C=A(BC).(AB)C = mnp + mpq,A(BC)=npq + mnq.$$mnp&lt;mnq,mpq&lt; npq$$所以 (AB)C 最小 首先，根据简单的矩阵知识，因为 AB ， A 的列数必须和 B 的行数相等。因此，排除C ;然后，再看 A 、 B 选项。在 A 选项中， mn 的矩阵 A 和 np 的矩阵 B 的乘积，得到 mp 的矩阵 AB ，而 AB 的每个元素需要 n 次乘法和 n-1 次加法，忽略加法，共需要 mnp 次乘法运算。同样情况分析 AB 之后再乘以 C 时的情况，共需要 mpq次乘法运算。因此， A 选项的(AB)C 需要的乘法次数是 mnp+mpq 。同理分析， B 选项的 A (BC)需要的乘法次数是 npq+mn*q 。 k-NN最近邻方法Q: 一般，k-NN最近邻方法在(B)的情况下效果较好A. 样本较多但典型性不好B. 样本较少但典型性好C. 样本呈团状分布D. 样本呈链状分布 A: 样本呈团状颇有迷惑性，这里应该指的是整个样本都是呈团状分布，这样kNN就发挥不出其求近邻的优势了，整体样本应该具有典型性好，样本较少，比较适宜。 常见分类方法（监督学习）：k-NN最近邻方法，支持向量机，决策树 kmeans Kmeans是聚类方法，典型的无监督学习方法 复习一下K-means算法，主要分为赋值阶段和更新阶段。 算法步骤： （1）随机选择K个点作为初始的质心 （2）将每个点指配到最近的质心 （3）重新计算簇的质心，直到质心不再发生变化。 K均值容易陷入局部最小值，无法表示类的形状，大小和宽度，是一种硬分类算法，针对它的这些缺点，提出了二分K均值和软K均值 CRF模型HMM和MEMM模型Q: 下列哪个不属于CRF模型对于HMM和MEMM模型的优势(B)A. 特征灵活B. 速度快C. 可容纳较多上下文信息D. 全局最优 CRF模型:条件随机场（Conditional Random Field，CRF）隐马尔可夫模型（Hidden Markov Model，HMM）最大熵马尔可夫模型（Maximum Entropy Markov Model，MEMM） CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） -与HMM比较 同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­——与MEMM比较 CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。—与ME比较 缺点：训练代价大、复杂度高 CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢 时间序列模型时间序列模型中,哪一个模型可以较好地拟合波动性的分析和预测（D） A. AR模型B. MA模型C. ARMA模型D. GARCH模型 AR模型：自回归模型，是一种线性模型MA模型：移动平均法模型，其中使用趋势移动平均法建立直线趋势的预测模型ARMA模型：自回归滑动平均模型，拟合较高阶模型GARCH模型：广义回归模型，对误差的方差建模，适用于波动性的分析和预测 AR模型是一种线性预测，即已知N个数据，可由模型推出第N点前面或后面的数据（设推出P点），所以其本质类似于插值。 MA模型(moving average model)滑动平均模型，模型参量法谱分析方法之一。 ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐。 GARCH模型称为广义ARCH模型，是ARCH模型的拓展,GARCH对误差的方差进行了进一步的建模，特别适用于波动性的分析和 预测。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测。 Naive Bayesian（NB）Q: 假定某同学使用Naive Bayesian（NB）分类模型时，不小心将训练数据的两个维度搞重复了，那么关于NB的说法中正确的是: 模型效果相比无重复特征的情况下精确度会降低 当两列特征高度相关时，无法用两列特征相同时所得到的结论来分析问题 A: NB的核心在于它假设向量的所有分量之间是独立的。 在贝叶斯理论系统中，都有一个重要的条件独立性假设：假设所有特征之间相互独立，这样才能将联合概率拆分. 主要原因就是由于存在重复的类别之后，破坏了原本的独立性假设。 Q: 位势函数法的积累势函数K(x)的作用相当于Bayes判决中的: 1.后验概率 2.类概率密度与先验概率的乘积(其实二者含义相同) 在贝叶斯决策中对于先验概率p(y)，分为已知和未知两种情况。 p(y)已知，直接使用贝叶斯公式求后验概率即可； p(y)未知，可以使用聂曼-皮尔逊决策(N-P决策)来计算决策面。 而最大最小损失规则主要就是使用解决最小损失规则时先验概率未知或难以计算的问题的。 高维数据进行降维: LASSO PCA 聚类分析 小波分析 线性判别法 拉普拉斯特征映射]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记（一）：NG的课程]]></title>
    <url>%2Farchives%2Fe6655ec0.html</url>
    <content type="text"><![CDATA[梯度下降 梯度下降的特点：梯度下降的结果一定会结束，也就是会收敛；但是收敛结果依赖于参数初始值。 局部最优值：当接近局部最小值时，步子会越来越小，直到快速收敛到全局最小值。 在局部最小值处，梯度是0；所以在接近局部最小值处，梯度越来越小，梯度下降每步变小。 Batch gradient descent批梯度下降 梯度下降算法的每一次迭代都要遍历整个训练集合。 每次迭代之后要更新参数，这对大规模数据集不利。 引入随机梯度下降或叫增量梯度下降。 注意到，这里激励函数不再用sigmoid{斜率也就是梯度近似都是0，难么梯度下降学习就会很慢}，而是用RULE。 Incremental gradient descent：随机梯度下降(一)好处： 为了开始修改参数，查看并利用第一个训练样本进行更新，这样依次使用第二个样本进行更新，不需要在调整之前便利所有训练数据集合。 对于特定或者普通的最小二乘回归问题，梯度下降可以给出参数向量的解析表达式，这样为了求参数的值就不需要进行迭代了。 (二)坏处： 不会精确的收敛到全局最小值，而是向着全局最小值附近徘徊，可能会一直徘徊。 但是通常情况下，得到的结果很接近全局最小值，特别是在大规模数据训练集时，至少比批梯度下降算法要快很多。 (三) 无人驾驶: 监督学习中的回归问题（梯度下降也是回归问题）。 汽车尝试预测表示行驶方向的连续变量的值。 局部加权回归locally weighted regression线性回归linear regression 逻辑回归logic regression 牛顿法 Underfitting欠拟合：数据中某些非常明显的模式没有被成功的拟合出来 Overfitting过拟合：算法拟合出的结果显示所给的特定数据的特质。而不是隐藏在其下的房屋价格随房屋大小变化的一般规律。 过小的特征集合使得模型过于简单；过大的特征集合使得模型过于复杂 parametric learning algorithm参数学习算法：有固定的参数的数目进行数据拟合的算法 Non-parametric learning algorithm无参数学习算法：参数数目随着训练集合的大小线性增长 局部加权线性回归locally weighted regression：特定的无参数学习算法缺点： 并不能完全避免过拟合和欠拟合的问题。 每次进行预测，要在一次根据整个训练集合拟合出。 若数据集大，代价高，但是可以KDtree来改善效率。 应用：用在了直升机自动驾驶上。 为什选择最小二乘回归作为误差项的估计方法？待更 似然性，中心极限定律待更 logic function=sigmoid function待更 感知器学习算法待更 牛顿学习法Newton’S Method 是一种不同的用来进行模型拟合的算法，例如可以对逻辑回归模型进行拟合。这类方法通常比梯度上升算法快得多。 收敛速度非常快，它的收敛速度用术语可以描述为：二次收敛。也就是，牛顿方法的每一次迭代都会使你正在逼近的解的有效数字的数目加倍。 通常需要迭代十几次就可以收敛，比上述梯度上升等算法要快得多 但是，每次迭代需要重新计算一次Hession矩阵的逆（n*n, n代表特征的数量） 因此，如果要处理的问题中有大量的特征，比如说几千个，那么Hession矩阵的逆的求解要花费很大的代价。 但对于规模较小，特征数量合理的很合适。 伯努利分布和高斯分布，都属于指数分布族 广义线性模型GLM，逻辑回归算法待更]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何提交博客]]></title>
    <url>%2Farchives%2F59ba8169.html</url>
    <content type="text"><![CDATA[如何提交博客第一步编辑markdown文档，在开头最前面加上1234567---title: sql基础语句categories: [sql,基础语句]tags: [sql,多个,选填]description: sql基础语句date: 2017-07-29 15:12:02--- 第1行是你的博客标题；第2行是文章分类，可以写多级分类，例子中的就是一级分类sql”下的二级分类”基础语句”；第4行是附加标签，有几个就填几个；第5行是文章描述；第6行是提交时间，按格式写就行；这里categories、tags、description都可以不写，空的话就不分配相应信息；注意每个冒号后空一个，上下的---不能掉 然后下一行开始写博客内容，完成后存储为xxxx.md格式。 第二步打开你的github仓库，选择xxx.github.io.source的一个进去打开source/_posts然后上传文件，填写commit的信息和描述，然后commit changes，等待上传完成 第三步博客上传好了，等待几分钟，你的博客就会提交成功，等待刷新成功，或者你可以登录AppVeyor，看你的上传进度，在console可以看到脚本的部署过程。]]></content>
      <categories>
        <category>技术文档</category>
      </categories>
  </entry>
</search>
